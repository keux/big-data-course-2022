{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Мини задание 3\n",
    "\n",
    "Продолжаем работать с задачкой на Kaggle про предсказание клика на рекламу - https://www.kaggle.com/c/avito-context-ad-clicks\n",
    "\n",
    "В этот раз нужно уже полноценно решить поставленную задачу.\n",
    "\n",
    "**1. [+5 баллов]** Из всех имеющихся данных необходимо составить датасет из **минимум 8 логических признаков**. Под логическим признаком имеется в виду то, что например кодирование категориального признака или текста в виде OheHot\\BoW не считается за кучу признаков - конкретный текст считается за 1 признак, категориальный признак считается за 1 признак.\n",
    "\n",
    "Всю работу по обработке данных и сборке финального датасета необходимо провести с помощью **спарка**.\n",
    "Финальный датасет должен быть закодирован в **формате Vowpal Wabbit**.\n",
    "\n",
    "**Важно** Следите за тем, чтобы исходное количество объектов не изменилось. При неаккуратной работе с join в sql вы можете начать терять какие-то записи, что плохо. Как минимум это означает, что вы не сможете собрать такие же признаки для тестовой выборки, которую предлагают в задаче.\n",
    "\n",
    "**Важно** Обратите внимание, что в задаче стоит вопрос именно про контекстуальную рекламу, а не про всю существующую.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\r\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f36b0df5970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.driver.maxResultSize\", \"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:12:50,246 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "2022-03-14 19:12:50,426 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2022-03-14 19:13:01,976 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2022-03-14 19:13:01,991 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"mhw3\", conf=conf)\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop  5350670676 2022-02-20 22:30 /user/avito/AdsInfo.tsv\r\n",
      "-rw-r--r--   1 ubuntu hadoop  9469373867 2022-02-20 22:26 /user/avito/SearchInfo.tsv\r\n",
      "-rw-r--r--   1 ubuntu hadoop 13180996392 2022-02-20 22:35 /user/avito/VisitsStream.tsv\r\n",
      "drwxr-xr-x   - ubuntu hadoop           0 2022-02-21 12:52 /user/avito/parquet\r\n",
      "drwxr-xr-x   - ubuntu hadoop           0 2022-02-21 18:56 /user/avito/results\r\n",
      "-rw-r--r--   1 ubuntu hadoop   557829937 2022-02-20 22:26 /user/avito/testSearchStream.tsv\r\n",
      "-rw-r--r--   1 ubuntu hadoop  7122681856 2022-02-20 22:29 /user/avito/trainSearchStream.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/avito/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2022-02-21 01:00 /user/avito/parquet/AdsInfo.parquet\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2022-02-21 00:32 /user/avito/parquet/SearchInfo.parquet\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2022-02-21 12:56 /user/avito/parquet/VisitsStream.parquet\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2022-02-21 00:42 /user/avito/parquet/trainSearchStream.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/avito/parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdsInfo.tsv\t PhoneRequestsStream.tsv       testSearchStream.tsv\r\n",
      "Category.tsv\t sampleSubmission.csv\t       trainSearchStream.tsv\r\n",
      "database.sqlite  sampleSubmission_HistCTR.csv  UserInfo.tsv\r\n",
      "Location.tsv\t SearchInfo.tsv\t\t       VisitsStream.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! ls  /home/ubuntu/avito-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put /home/ubuntu/avito-dataset/Category.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/Location.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/PhoneRequestsStream.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/UserInfo.tsv /user/avito/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df(name_df):\n",
    "    df = se.read.option(\"mode\",\"DROPMALFORMED\") \\\n",
    "                .option('sep', \"\\t\") \\\n",
    "                .csv(\"/user/avito/{}.tsv\".format(name_df), header=True, inferSchema=True)\n",
    "    df.write.parquet(\"/user/avito/parquet/{}.parquet\".format(name_df))\n",
    "\n",
    "# write_df('Category')\n",
    "# write_df('Location')\n",
    "# write_df('PhoneRequestsStream')\n",
    "# write_df('UserInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/avito/testSearchStream.tsv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put /home/ubuntu/avito-dataset/testSearchStream.tsv /user/avito/\n",
    "write_df('testSearchStream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "search_info = se.read.parquet(\"/user/avito/parquet/SearchInfo.parquet\")\n",
    "search_info.registerTempTable('search_info')\n",
    "\n",
    "stream_df = se.read.parquet(\"/user/avito/parquet/trainSearchStream.parquet\")\n",
    "stream_df.registerTempTable('stream_train')\n",
    "\n",
    "stream_test_df = se.read.parquet(\"/user/avito/parquet/testSearchStream.parquet\")\n",
    "stream_test_df.registerTempTable('stream_test')\n",
    "\n",
    "ads_df = se.read.parquet(\"/user/avito/parquet/AdsInfo.parquet\")\n",
    "ads_df.registerTempTable('ads')\n",
    "\n",
    "# visits_df = se.read.parquet(\"/user/avito/parquet/VisitsStream.parquet\")\n",
    "# visits_df.registerTempTable('visits')\n",
    "\n",
    "category_df = se.read.parquet(\"/user/avito/parquet/Category.parquet\")\n",
    "category_df.registerTempTable('category')\n",
    "\n",
    "location_df = se.read.parquet(\"/user/avito/parquet/Location.parquet\")\n",
    "location_df.registerTempTable('location')\n",
    "\n",
    "# phone_df = se.read.parquet(\"/user/avito/parquet/PhoneRequestsStream.parquet\")\n",
    "# phone_df.registerTempTable('phones')\n",
    "\n",
    "# user_df = se.read.parquet(\"/user/avito/parquet/UserInfo.parquet\")\n",
    "# user_df.registerTempTable('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:13:39,051 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020]\n",
      "2022-03-14 19:13:39,057 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2022-03-14 19:13:39,058 INFO balancer.Balancer: included nodes = []\n",
      "2022-03-14 19:13:39,058 INFO balancer.Balancer: excluded nodes = []\n",
      "2022-03-14 19:13:39,058 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2022-03-14 19:13:39,063 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2022-03-14 19:13:40,366 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2022-03-14 19:13:40,374 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2022-03-14 19:13:40,374 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2022-03-14 19:13:40,402 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.35:9866\n",
      "2022-03-14 19:13:40,404 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2022-03-14 19:13:40,404 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Mar 14, 2022 7:13:40 PM           0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020\n",
      "Mar 14, 2022 7:13:40 PM  Balancing took 1.963 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174767397"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM stream_train\n",
    "    WHERE IsClick IS NOT NULL\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2859797778.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [32]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Full Ad: (4)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# HistCTR \n",
    "# Position\n",
    "# ObjectType\n",
    "\n",
    "# Ads Title\n",
    "# Ads Price\n",
    "\n",
    "# SearchQuery\n",
    "\n",
    "Full Ad: (4)\n",
    "    AdID, CatLevel, Price, Title\n",
    "\n",
    "    CatID -> Cat Level\n",
    "\n",
    "    без params\n",
    "\n",
    "SearchInfo: (4)\n",
    "    SearchID, hours, searchQuery, searchCatLevel\n",
    "    \n",
    "    searchDate -> hours\n",
    "    serachDate -> weekday\n",
    "    searchCat -> searchCatLevel\n",
    "    SearchQuery\n",
    "    \n",
    "full stream:\n",
    "    HistCTR, Position, ObjectType, Price, Title, hours, searchQuery, searchCatLevel, CatLevel\n",
    "    \n",
    "\n",
    "____\n",
    "\n",
    "# number of phone clicks for an add\n",
    "# time of the day\n",
    "# количество просмотров ad\n",
    "\n",
    "# Ad Location\n",
    "\n",
    "# level of category\n",
    "# average price for given category views of a given user\n",
    "\n",
    "____\n",
    "\n",
    "# # of ads seen up to a given impression\n",
    "# # of ads clicked up to a given impression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оставляем только контекстную рекламу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream_df.filter('IsClick IS NOT NULL').cache()\n",
    "stream_df.registerTempTable('stream_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_df = ads_df.filter(ads_df.IsContext == 1).cache()\n",
    "ads_df.registerTempTable('ads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdID</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>Params</th>\n",
       "      <th>Price</th>\n",
       "      <th>Title</th>\n",
       "      <th>IsContext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>{181:'Для офиса'}</td>\n",
       "      <td>69999.0</td>\n",
       "      <td>Кабинет руководителя премиум-класса Han Италия</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>{45:'Компьютерные столы и кресла'}</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>Стол двухтумбовый Милан-10 - венге-клен</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>{156:'Горные'}</td>\n",
       "      <td>33900.0</td>\n",
       "      <td>\"1412 FORMAT 26\"\" 24 СК\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>{45:'Шкафы и комоды'}</td>\n",
       "      <td>12090.0</td>\n",
       "      <td>Детский комплекс Дюймовочка-3.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>{178:'Для девочек', 179:'Трикотаж'}</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>Свитер</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>{178:'Для девочек', 179:'Другое'}</td>\n",
       "      <td>539.0</td>\n",
       "      <td>Купальник цельный</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>{45:'Столы и стулья'}</td>\n",
       "      <td>17980.0</td>\n",
       "      <td>Стол для переговоров прямоугольный Шпон Венге</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>{45:'Подставки и тумбы'}</td>\n",
       "      <td>1398.0</td>\n",
       "      <td>\"Полка для обуви \"\"4-Tier Wood Stackable\"\"\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>{44:'Инструменты'}</td>\n",
       "      <td>12313.0</td>\n",
       "      <td>Защитная гильза вибротрамбовки Masalta MR75R</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>{45:'Шкафы и комоды'}</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>Прихожая Мебелайн-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdID  LocationID  CategoryID                               Params    Price  \\\n",
       "0   473         NaN          53                    {181:'Для офиса'}  69999.0   \n",
       "1  1515         NaN          38   {45:'Компьютерные столы и кресла'}   8300.0   \n",
       "2  2247         NaN           4                       {156:'Горные'}  33900.0   \n",
       "3  3892         NaN          38                {45:'Шкафы и комоды'}  12090.0   \n",
       "4  4223         NaN          60  {178:'Для девочек', 179:'Трикотаж'}   1304.0   \n",
       "5  5094         NaN          60    {178:'Для девочек', 179:'Другое'}    539.0   \n",
       "6  5708         NaN          38                {45:'Столы и стулья'}  17980.0   \n",
       "7  7445         NaN          38             {45:'Подставки и тумбы'}   1398.0   \n",
       "8  8370         NaN          50                   {44:'Инструменты'}  12313.0   \n",
       "9  8790         NaN          38                {45:'Шкафы и комоды'}   8900.0   \n",
       "\n",
       "                                            Title  IsContext  \n",
       "0  Кабинет руководителя премиум-класса Han Италия          1  \n",
       "1         Стол двухтумбовый Милан-10 - венге-клен          1  \n",
       "2                        \"1412 FORMAT 26\"\" 24 СК\"          1  \n",
       "3                 Детский комплекс Дюймовочка-3.4          1  \n",
       "4                                          Свитер          1  \n",
       "5                               Купальник цельный          1  \n",
       "6   Стол для переговоров прямоугольный Шпон Венге          1  \n",
       "7     \"Полка для обуви \"\"4-Tier Wood Stackable\"\"\"          1  \n",
       "8    Защитная гильза вибротрамбовки Masalta MR75R          1  \n",
       "9                             Прихожая Мебелайн-1          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.sql(\"\"\"\n",
    "    select *\n",
    "    from ads\n",
    "    limit 10\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdsInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "import string\n",
    "\n",
    "def wordify(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    pattern = re.compile(r\"[\\w]+\")\n",
    "    result = []\n",
    "\n",
    "    for match in pattern.finditer(text.lower()):\n",
    "        word = match.group(0)\n",
    "        result.append(word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "f_wordify = se.udf.register(\"wordify\", wordify, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_full = se.sql(\"\"\"\n",
    "    SELECT\n",
    "        l.AdID,\n",
    "        r.Level as CatLevel,\n",
    "        l.Price,\n",
    "        wordify(l.Title) as Title\n",
    "    FROM ads l\n",
    "    JOIN category r\n",
    "    ON l.CategoryID = r.CategoryID\n",
    "\"\"\").cache()\n",
    "ads_full.registerTempTable('ads_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdID</th>\n",
       "      <th>CatLevel</th>\n",
       "      <th>Price</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>473</td>\n",
       "      <td>3</td>\n",
       "      <td>69999.0</td>\n",
       "      <td>кабинет руководителя премиум класса han италия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515</td>\n",
       "      <td>3</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>стол двухтумбовый милан 10 венге клен</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2247</td>\n",
       "      <td>3</td>\n",
       "      <td>33900.0</td>\n",
       "      <td>1412 format 26 24 ск</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3892</td>\n",
       "      <td>3</td>\n",
       "      <td>12090.0</td>\n",
       "      <td>детский комплекс дюймовочка 3 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4223</td>\n",
       "      <td>3</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>свитер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28531</th>\n",
       "      <td>36890203</td>\n",
       "      <td>3</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>жанровая картина безмятежность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28532</th>\n",
       "      <td>36890207</td>\n",
       "      <td>3</td>\n",
       "      <td>20836.0</td>\n",
       "      <td>кухонный уголок остин т</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28533</th>\n",
       "      <td>36890301</td>\n",
       "      <td>3</td>\n",
       "      <td>49990.0</td>\n",
       "      <td>музыкальная система midi panasonic sc max 770gsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28534</th>\n",
       "      <td>36891249</td>\n",
       "      <td>3</td>\n",
       "      <td>5999.0</td>\n",
       "      <td>стол триал mos f1073 черный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28535</th>\n",
       "      <td>36891660</td>\n",
       "      <td>3</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>аккумулятор lenovo g580 b580 b480 b485 b585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28536 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AdID  CatLevel    Price  \\\n",
       "0           473         3  69999.0   \n",
       "1          1515         3   8300.0   \n",
       "2          2247         3  33900.0   \n",
       "3          3892         3  12090.0   \n",
       "4          4223         3   1304.0   \n",
       "...         ...       ...      ...   \n",
       "28531  36890203         3  50000.0   \n",
       "28532  36890207         3  20836.0   \n",
       "28533  36890301         3  49990.0   \n",
       "28534  36891249         3   5999.0   \n",
       "28535  36891660         3   2600.0   \n",
       "\n",
       "                                                  Title  \n",
       "0        кабинет руководителя премиум класса han италия  \n",
       "1                 стол двухтумбовый милан 10 венге клен  \n",
       "2                                  1412 format 26 24 ск  \n",
       "3                       детский комплекс дюймовочка 3 4  \n",
       "4                                                свитер  \n",
       "...                                                 ...  \n",
       "28531                    жанровая картина безмятежность  \n",
       "28532                           кухонный уголок остин т  \n",
       "28533  музыкальная система midi panasonic sc max 770gsk  \n",
       "28534                       стол триал mos f1073 черный  \n",
       "28535       аккумулятор lenovo g580 b580 b480 b485 b585  \n",
       "\n",
       "[28536 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads_full.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SearchInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def take_hours(text):\n",
    "    cur_date = datetime.strptime(text, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    hours = cur_date.strftime('%H')\n",
    "    return hours\n",
    "\n",
    "def take_weekday(text):\n",
    "    cur_date = datetime.strptime(text, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    weekday = cur_date.weekday()\n",
    "    return weekday\n",
    "\n",
    "take_hours = se.udf.register(\"take_hours\", take_hours)\n",
    "take_weekday = se.udf.register(\"take_weekday\", take_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:15:23,438 WARN execution.CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "search_full = se.sql(\"\"\"\n",
    "    SELECT\n",
    "        l.SearchID,\n",
    "        take_hours(l.SearchDate) as Hour,\n",
    "        take_weekday(l.SearchDate) as Weekday,\n",
    "        wordify(l.SearchQuery) as Query,\n",
    "        r.Level as SearchCatLevel\n",
    "    FROM search_info l\n",
    "    JOIN category r\n",
    "    ON l.CategoryID = r.CategoryID\n",
    "\"\"\").cache()\n",
    "search_full.registerTempTable('search_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = se.sql(\"\"\"\n",
    "    WITH stream_with_ad AS (\n",
    "        SELECT\n",
    "            l.SearchID,\n",
    "            l.Position,\n",
    "            l.ObjectType,\n",
    "            l.HistCTR,\n",
    "            l.IsClick,\n",
    "            r.CatLevel,\n",
    "            r.Price,\n",
    "            r.Title\n",
    "        FROM stream_train l\n",
    "        LEFT JOIN ads_full r \n",
    "        ON l.AdID = r.AdID\n",
    "    )\n",
    "    SELECT\n",
    "        sl.Position,\n",
    "        sl.ObjectType,\n",
    "        sl.HistCTR,\n",
    "        sl.CatLevel,\n",
    "        sl.Price,\n",
    "        sl.Title,\n",
    "        sr.Hour,\n",
    "        sr.Weekday,\n",
    "        sr.Query,\n",
    "        sr.SearchCatLevel,\n",
    "        sl.IsClick\n",
    "    FROM stream_with_ad sl\n",
    "    LEFT JOIN search_full sr\n",
    "    ON sl.SearchID = sr.SearchID\n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------+--------+--------------------+----+-------+-----+--------------+-------+\n",
      "|Position|ObjectType| HistCTR|CatLevel|   Price|               Title|Hour|Weekday|Query|SearchCatLevel|IsClick|\n",
      "+--------+----------+--------+--------+--------+--------------------+----+-------+-----+--------------+-------+\n",
      "|       1|         3|  1.0E-5|       3|   671.0|футболка поло с к...|  17|      1|     |             3|      0|\n",
      "|       7|         3|  1.0E-5|       3|  2639.0|              куртка|  17|      1|     |             3|      0|\n",
      "|       1|         3| 0.00634|       3| 12870.0|золотые серьги so...|  18|      6|     |             3|      0|\n",
      "|       7|         3|0.008191|       3|  6500.0|золотое кольцо но...|  18|      6|     |             3|      0|\n",
      "|       1|         3|0.012376|       3| 36600.0|детская кровать т...|  20|      2|     |             3|      0|\n",
      "|       7|         3|0.006342|       3|  1890.0|комплект на выпис...|  20|      2|     |             3|      0|\n",
      "|       7|         3|0.001749|       3|     1.0|контрактный двига...|  19|      4|     |             3|      0|\n",
      "|       1|         3|0.007477|       3|  9000.0|дверь задняя прав...|  19|      4|     |             3|      0|\n",
      "|       7|         3|0.041266|       3|  2600.0|бампер передний п...|  15|      1|     |             2|      0|\n",
      "|       1|         3|0.002475|       3|165000.0|новые акпп опель ...|  15|      1|     |             2|      0|\n",
      "|       1|         3|0.002926|       3|  4590.0|прогулочная коляс...|  22|      4|     |             3|      0|\n",
      "|       1|         3|0.003284|       3|  1291.0|растение научград...|  17|      0|     |             3|      0|\n",
      "|       1|         3| 7.84E-4|       3|  2190.0|леггинсы lost ink...|  12|      4|     |             3|      0|\n",
      "|       7|         3| 8.46E-4|       3|104900.0|промышленный прин...|  19|      5|     |             3|      0|\n",
      "|       1|         3|  1.0E-5|       3| 51000.0|виброплита vektor...|  19|      5|     |             3|      0|\n",
      "|       1|         3|0.045343|       3|  9890.0|автомобиль carmel...|  19|      1|     |             3|      0|\n",
      "|       7|         3|0.005326|       3| 39500.0|   самокат для детей|  19|      1|     |             3|      0|\n",
      "|       1|         3|0.006211|       3|   999.0|    пижама с шортами|  12|      1|     |             3|      0|\n",
      "|       7|         3|0.004095|       3|   990.0|                кеды|  12|      1|     |             3|      0|\n",
      "|       1|         3|0.009272|       3|   952.0|набор для капельн...|  18|      6|     |             3|      0|\n",
      "+--------+----------+--------+--------+--------+--------------------+----+-------+-----+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Position=7, ObjectType=3, HistCTR=1e-05, CatLevel=3, Price=3700.0, Title='Беспроводная клавиатура APPLE Wireless Keyboard Bl', Hour='23', Weekday='1', Query='компьютеры', SearchCatLevel=1, IsClick=0),\n",
       " Row(Position=1, ObjectType=3, HistCTR=0.029126, CatLevel=3, Price=34990.0, Title='Apple Mac mini (MGEM2RU/A)', Hour='23', Weekday='1', Query='компьютеры', SearchCatLevel=1, IsClick=0),\n",
       " Row(Position=1, ObjectType=3, HistCTR=0.005822, CatLevel=3, Price=10010.0, Title='Телевизор LED Mystery MTV-3217LW черный', Hour='20', Weekday='4', Query='led телевизор бу', SearchCatLevel=3, IsClick=0),\n",
       " Row(Position=7, ObjectType=3, HistCTR=0.000465, CatLevel=3, Price=44480.0, Title='LED телевизор Philips 49PUS7809/60', Hour='20', Weekday='4', Query='led телевизор бу', SearchCatLevel=3, IsClick=0),\n",
       " Row(Position=1, ObjectType=3, HistCTR=0.040747, CatLevel=3, Price=4450.0, Title='Инкубатор Несушка БИ-2', Hour='12', Weekday='4', Query='инкубатор', SearchCatLevel=1, IsClick=0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.filter(train_df.Query != \"\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vw(row):\n",
    "    target = row['IsClick']\n",
    "    \n",
    "    title = wordify(row['Title'])\n",
    "    query = row['Query'] if row['Query'] else \"nan\"\n",
    "\n",
    "    template = \"{target} \\\n",
    "    |q {query} \\\n",
    "    |t {title} \\\n",
    "    |pos Position:{position} \\\n",
    "    |ot ObjectType:{objectType} \\\n",
    "    |hi HistCTR:{histCTR}\\\n",
    "    |cl CatLevel:{catLevel} \\\n",
    "    |ho Hour:{hour}\\\n",
    "    |w Weekday:{weekday}\\\n",
    "    |sl SearchCatLevel:{searchCatLevel}\\\n",
    "    |pr Price:{price}\"\n",
    "    return template.format(\n",
    "        target=target,\n",
    "        query=query,\n",
    "        title=title,\n",
    "        position=row['Position'],\n",
    "        objectType=row['ObjectType'],\n",
    "        histCTR=row['HistCTR'],\n",
    "        catLevel=row['CatLevel'],\n",
    "        hour=row['Hour'],\n",
    "        weekday=row['Weekday'],\n",
    "        searchCatLevel=row['SearchCatLevel'],\n",
    "        price=row['Price']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0     |q nan     |t футболка поло с короткими рукавами     |pos Position:1     |ot ObjectType:3     |hi HistCTR:1e-05    |cl CatLevel:3     |ho Hour:17    |w Weekday:1    |sl SearchCatLevel:3    |pr Price:671.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_vw(train_df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/avito/datasets/train.vw\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/avito/datasets/train.vw || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_df\n",
    "    .rdd\n",
    "    .map(lambda x: convert_to_vw(x))\n",
    "    .saveAsTextFile('/user/avito/datasets/train.vw')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/avito/datasets/train.vw/* >> train_df.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. number of phone clicks for an add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 10:15:44,243 WARN execution.CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "ad_phoned = phone_df.select(phone_df.AdID, phone_df.PhoneRequestDate).groupBy(phone_df.AdID).count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_phoned.join(ads_df, ad_phoned.AdID == ads_df.AdID).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- IPID: integer (nullable = true)\n",
      " |-- AdID: integer (nullable = true)\n",
      " |-- ViewDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. number of views for an add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = se.sql(\"\"\"\n",
    "    SELECT\n",
    "    l.AdID AS AdID,\n",
    "    count(r.ViewDate) AS ViewsCnt\n",
    "    FROM ads l\n",
    "    JOIN visits r\n",
    "    ON l.AdID = r.AdID\n",
    "    GROUP BY l.AdID\n",
    "\"\"\").cache()\n",
    "views_df.registerTempTable('views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|AdID|ViewsCnt|\n",
      "+----+--------+\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:===============================================>        (63 + 5) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "views_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------------------+-------+--------------------+---------+\n",
      "| AdID|LocationID|CategoryID|              Params|  Price|               Title|IsContext|\n",
      "+-----+----------+----------+--------------------+-------+--------------------+---------+\n",
      "|  473|      null|        53|   {181:'Для офиса'}|69999.0|Кабинет руководит...|        1|\n",
      "| 1515|      null|        38|{45:'Компьютерные...| 8300.0|Стол двухтумбовый...|        1|\n",
      "| 2247|      null|         4|      {156:'Горные'}|33900.0|\"1412 FORMAT 26\"\"...|        1|\n",
      "| 3892|      null|        38|{45:'Шкафы и комо...|12090.0|Детский комплекс ...|        1|\n",
      "| 4223|      null|        60|{178:'Для девочек...| 1304.0|              Свитер|        1|\n",
      "| 5094|      null|        60|{178:'Для девочек...|  539.0|   Купальник цельный|        1|\n",
      "| 5708|      null|        38|{45:'Столы и стул...|17980.0|Стол для перегово...|        1|\n",
      "| 7445|      null|        38|{45:'Подставки и ...| 1398.0|\"Полка для обуви ...|        1|\n",
      "| 8370|      null|        50|  {44:'Инструменты'}|12313.0|Защитная гильза в...|        1|\n",
      "| 8790|      null|        38|{45:'Шкафы и комо...| 8900.0| Прихожая Мебелайн-1|        1|\n",
      "| 9522|      null|        26|{132:'Телевизоры ...|42042.0|Телевизор Toshiba...|        1|\n",
      "| 9721|      null|        60|{178:'Для девочек...|  689.0|            Футболка|        1|\n",
      "|10282|      null|        34|    {5:'Экипировка'}| 2290.0|Мотошлем открытый...|        1|\n",
      "|11568|      null|        50|{44:'Стройматериа...|  390.0|Ламинат Kastamonu...|        1|\n",
      "|12008|      null|        60|{178:'Для девочек...| 3346.0|Комплект верхней ...|        1|\n",
      "|12279|      null|      null|                null|   null|                null|        1|\n",
      "|13978|      null|        50|{44:'Стройматериа...| 1994.0|Паркетная доска -...|        1|\n",
      "|15122|      null|        44|{223:'Компактные ...|14990.0|Фотоаппарат компа...|        1|\n",
      "|15464|      null|        47|{127:'Велосипеды ...| 4125.0|Детский велосипед...|        1|\n",
      "|17091|      null|        34|{5:'Запчасти', 59...| 6810.0|Запчасти б/у и но...|        1|\n",
      "+-----+----------+----------+--------------------+-------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+-----+-----+---------+------+----+----+--------+\n",
      "|AdID|LocationID|CategoryID|Params|Price|Title|IsContext|UserID|IPID|AdID|ViewDate|\n",
      "+----+----------+----------+------+-----+-----+---------+------+----+----+--------+\n",
      "+----+----------+----------+------+-----+-----+---------+------+----+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "se.sql(\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM ads\n",
    "    JOIN visits ON ads.AdID = visits.AdID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "| UserID|   IPID|    AdID|            ViewDate|\n",
      "+-------+-------+--------+--------------------+\n",
      "|3339459|2141522|17013380|2015-05-19 17:47:...|\n",
      "|3472056| 468478| 9250676|2015-05-19 17:47:...|\n",
      "|3498500|2194994|16265489|2015-05-19 17:47:...|\n",
      "|3581440| 167293|29431869|2015-05-19 17:47:...|\n",
      "|3756375|1294847|  311552|2015-05-19 17:47:...|\n",
      "|3891751|1190064| 9709139|2015-05-19 17:47:...|\n",
      "|3971739|1811955|16457203|2015-05-19 17:47:...|\n",
      "|4006974|1796040|36742059|2015-05-19 17:47:...|\n",
      "|4102599| 678975|32139132|2015-05-19 17:47:...|\n",
      "|4106979|1479340|32656590|2015-05-19 17:47:...|\n",
      "|4203357|1759496| 2785886|2015-05-19 17:47:...|\n",
      "|4271481| 604680| 2311288|2015-05-19 17:47:...|\n",
      "|  29832|1519127| 2774307|2015-05-19 17:47:...|\n",
      "| 106382| 698275|24729353|2015-05-19 17:47:...|\n",
      "| 232031| 351316| 3087932|2015-05-19 17:47:...|\n",
      "| 263551|1706098| 3345028|2015-05-19 17:47:...|\n",
      "| 296159|1597739| 9021206|2015-05-19 17:47:...|\n",
      "| 323952| 290722|  974185|2015-05-19 17:47:...|\n",
      "| 384101|1192160|16225283|2015-05-19 17:47:...|\n",
      "| 399210| 909600|25996922|2015-05-19 17:47:...|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average price for given category views of a given user\n",
    "# avg_price = search_info.join(ads_df, search_info.CategoryID == ads_df.CategoryID).select(search_info.UserID, ads_df.CategoryID, ads_df.Price)\n",
    "# avg_price = avg_price.groupBy('UserID', 'CategoryID').agg({\"Price\": \"avg\"}).cache()\n",
    "# avg_price.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time of the day\n",
    "# # day of the week\n",
    "\n",
    "# from datetime import datetime\n",
    "# def take_hours(text):\n",
    "#     cur_date = datetime.strptime(text, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "#     hours = cur_date.strftime('%H')\n",
    "#     return hours\n",
    "\n",
    "# def take_weekday(text):\n",
    "#     cur_date = datetime.strptime(text, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "#     weekday = cur_date.weekday()\n",
    "#     return weekday\n",
    "\n",
    "# take_hours = se.udf.register(\"take_hours\", take_hours)\n",
    "# take_weekday = se.udf.register(\"take_weekday\", take_weekday)\n",
    "\n",
    "# visits_days = se.sql(\"\"\"\n",
    "#     SELECT DISTINCT\n",
    "#         UserID,\n",
    "#         take_hours(ViewDate) as Hour,\n",
    "#     FROM visits\n",
    "# \"\"\").cache()\n",
    "# visits_days.registerTempTable(\"days\")\n",
    "# visits_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SearchID: integer (nullable = true)\n",
      " |-- SearchDate: string (nullable = true)\n",
      " |-- IPID: integer (nullable = true)\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- IsUserLoggedOn: integer (nullable = true)\n",
      " |-- SearchQuery: string (nullable = true)\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- SearchParams: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- SearchID: integer (nullable = true)\n",
      " |-- AdID: integer (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- ObjectType: integer (nullable = true)\n",
      " |-- HistCTR: double (nullable = true)\n",
      " |-- IsClick: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_info.printSchema()\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream_df.filter(\"IsClick IS NOT NULL\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_with_query = stream_df.join(search_info, search_info.SearchID == stream_df.SearchID, 'left')\n",
    "stream_with_query = stream_with_query.select('SearchQuery', 'AdID', 'Position', 'ObjectType', 'HistCTR', 'IsClick').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SearchQuery: string (nullable = true)\n",
      " |-- AdID: integer (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- ObjectType: integer (nullable = true)\n",
      " |-- HistCTR: double (nullable = true)\n",
      " |-- IsClick: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_with_query.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AdID: integer (nullable = true)\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- Params: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- IsContext: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_with_phone = ads_df.join(ad_phoned, ads_df.AdID == ad_phoned.AdID, 'left').select(ads_df.AdID, 'Price', 'Title', 'IsContext', 'count').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AdID: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- IsContext: integer (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_with_phone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = stream_with_query.join(ads_with_phone, ads_with_phone.AdID == stream_with_query.AdID, 'left')\n",
    "train_df = train_df.select('SearchQuery', 'Position', 'ObjectType', 'HistCTR', 'Price', 'Title', 'IsContext', 'count', 'IsClick').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SearchQuery: string (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- ObjectType: integer (nullable = true)\n",
      " |-- HistCTR: double (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- IsContext: integer (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- IsClick: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.na.fill({'count':0, 'SearchQuery': \"\"}).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+-------+-----+-----+---------+-----+-------+\n",
      "|SearchQuery|Position|ObjectType|HistCTR|Price|Title|IsContext|count|IsClick|\n",
      "+-----------+--------+----------+-------+-----+-----+---------+-----+-------+\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|   комплект|       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|   комплект|       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|   комплект|       1|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|   комплект|       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|   комплект|       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "|           |       7|         3| 1.0E-5|458.0|2топа|        1|    0|      0|\n",
      "+-----------+--------+----------+-------+-----+-----+---------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_test_with_query = stream_test_df.join(search_info, search_info.SearchID == stream_test_df.SearchID, 'left')\n",
    "stream_test_with_query = stream_test_with_query.select('SearchQuery', 'AdID', 'Position', 'ObjectType', 'HistCTR').cache()\n",
    "\n",
    "test_df = stream_test_with_query.join(ads_with_phone, ads_with_phone.AdID == stream_test_with_query.AdID, 'left')\n",
    "test_df = test_df.select('SearchQuery', 'Position', 'ObjectType', 'HistCTR', 'Price', 'Title', 'IsContext', 'count')\n",
    "test_df = test_df.na.fill({'count':0, 'SearchQuery': \"\"}).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+-------+---------+--------------------+---------+-----+\n",
      "|         SearchQuery|Position|ObjectType|HistCTR|    Price|               Title|IsContext|count|\n",
      "+--------------------+--------+----------+-------+---------+--------------------+---------+-----+\n",
      "|                    |       8|         1|   null|1600000.0|Камаз 65117 зерновоз|        0|    1|\n",
      "|              нексия|       2|         1|   null|   1050.0|Фонарь задний Дэу...|        0|    0|\n",
      "|              нексия|       8|         1|   null|   1050.0|Фонарь задний Дэу...|        0|    0|\n",
      "|                    |       6|         1|   null|   8000.0|Продам резину на УАЗ|        0|    0|\n",
      "|                    |       8|         1|   null|   8000.0|Продам резину на УАЗ|        0|    0|\n",
      "|                 уаз|       8|         1|   null|   8000.0|Продам резину на УАЗ|        0|    0|\n",
      "|                    |       6|         1|   null| 200000.0|  Ваш надежный мопед|        0|    0|\n",
      "|                    |       8|         1|   null|  70250.0|Stevens Applebee ...|        0|    0|\n",
      "|           велосипед|       8|         1|   null|  70250.0|Stevens Applebee ...|        0|    0|\n",
      "|куртка для береме...|       7|         1|   null|   1500.0|Стильное пальто д...|        0|    0|\n",
      "|                    |       6|         1|   null|  20500.0|           255/35/18|        0|    0|\n",
      "|              кимоно|       8|         1|   null|   2300.0|Обидома япония Се...|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "|                    |       2|         2|   null|  22500.0|       Колеса летние|        0|    0|\n",
      "+--------------------+--------+----------+-------+---------+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(use_memory_fs=False, progress_bar=True, nb_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_train = train_df.agg({'Price': 'avg'}).collect()[0]['avg(Price)']\n",
    "avg_price_test = test_df.agg({'Price': 'avg'}).collect()[0]['avg(Price)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.na.fill({'HistCTR':0, 'Title': \"\", 'Price': avg_price_train}).cache()\n",
    "test_df = test_df.na.fill({'HistCTR':0, 'Title': \"\", 'Price': avg_price_test}).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write.parquet(\"/user/avito/parquet/train_df_2.parquet\")\n",
    "test_df.write.parquet(\"/user/avito/parquet/test_df_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df = se.read.parquet(\"/user/avito/parquet/train_df_2.parquet\").cache()\n",
    "test_df = se.read.parquet(\"/user/avito/parquet/test_df_2.parquet\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-12 16:09:17--  http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624\n",
      "Resolving finance.yendor.com (finance.yendor.com)... 69.163.152.190\n",
      "Connecting to finance.yendor.com (finance.yendor.com)|69.163.152.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9467376 (9.0M)\n",
      "Saving to: ‘/usr/bin/vw’\n",
      "\n",
      "/usr/bin/vw         100%[===================>]   9.03M  4.15MB/s    in 2.2s    \n",
      "\n",
      "2022-03-12 16:09:20 (4.15 MB/s) - ‘/usr/bin/vw’ saved [9467376/9467376]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! sudo wget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O /usr/bin/vw\n",
    "! sudo chmod +x /usr/bin/vw\n",
    "! sudo chown ubuntu /usr/bin/vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://storage.yandexcloud.net/dataproc2x/ci/2-0-alpha-12 focal InRelease\n",
      "Hit:2 https://repo.saltproject.io/py3/ubuntu/20.04/amd64/3002 focal InRelease  \n",
      "Hit:3 http://mirror.yandex.ru/ubuntu focal InRelease                           \n",
      "Get:4 http://mirror.yandex.ru/ubuntu focal-updates InRelease [114 kB]          \n",
      "Get:5 https://repos.influxdata.com/ubuntu focal InRelease [4,736 B]            \n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Get:7 http://mirror.yandex.ru/ubuntu focal-backports InRelease [108 kB]        \n",
      "Get:8 http://mirror.yandex.ru/mirrors/postgresql focal-pgdg InRelease [86.6 kB]\n",
      "Get:9 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 Packages [1,641 kB]\n",
      "Get:10 https://repos.influxdata.com/ubuntu focal/stable amd64 Packages [1,267 B]\n",
      "Get:11 http://mirror.yandex.ru/ubuntu focal-updates/main i386 Packages [616 kB]\n",
      "Get:12 http://mirror.yandex.ru/ubuntu focal-updates/main Translation-en [312 kB]\n",
      "Get:13 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 c-n-f Metadata [14.8 kB]\n",
      "Get:14 http://mirror.yandex.ru/ubuntu focal-updates/restricted amd64 Packages [853 kB]\n",
      "Get:15 http://mirror.yandex.ru/ubuntu focal-updates/restricted i386 Packages [23.1 kB]\n",
      "Get:16 http://mirror.yandex.ru/ubuntu focal-updates/restricted Translation-en [122 kB]\n",
      "Get:17 http://mirror.yandex.ru/ubuntu focal-updates/restricted amd64 c-n-f Metadata [500 B]\n",
      "Get:18 http://mirror.yandex.ru/ubuntu focal-updates/universe i386 Packages [673 kB]\n",
      "Get:19 http://mirror.yandex.ru/ubuntu focal-updates/universe amd64 Packages [909 kB]\n",
      "Get:20 https://repos.influxdata.com/ubuntu focal/stable i386 Packages [932 B]  \n",
      "Get:21 https://packages.fluentbit.io/ubuntu/focal focal InRelease [10.6 kB]    \n",
      "Get:22 http://mirror.yandex.ru/ubuntu focal-updates/universe Translation-en [202 kB]\n",
      "Get:23 http://mirror.yandex.ru/ubuntu focal-updates/universe amd64 c-n-f Metadata [20.2 kB]\n",
      "Get:24 http://mirror.yandex.ru/ubuntu focal-updates/multiverse amd64 Packages [23.8 kB]\n",
      "Get:25 http://mirror.yandex.ru/ubuntu focal-updates/multiverse i386 Packages [8,460 B]\n",
      "Get:26 http://mirror.yandex.ru/ubuntu focal-updates/multiverse Translation-en [7,312 B]\n",
      "Get:27 http://mirror.yandex.ru/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [580 B]\n",
      "Get:28 http://mirror.yandex.ru/ubuntu focal-backports/main amd64 Packages [42.2 kB]\n",
      "Get:29 http://mirror.yandex.ru/ubuntu focal-backports/main i386 Packages [34.8 kB]\n",
      "Get:30 http://mirror.yandex.ru/ubuntu focal-backports/main Translation-en [10.1 kB]\n",
      "Get:31 http://mirror.yandex.ru/ubuntu focal-backports/universe amd64 Packages [22.7 kB]\n",
      "Get:32 http://mirror.yandex.ru/ubuntu focal-backports/universe i386 Packages [12.9 kB]\n",
      "Get:33 http://mirror.yandex.ru/ubuntu focal-backports/universe Translation-en [15.4 kB]\n",
      "Get:34 http://mirror.yandex.ru/ubuntu focal-backports/universe amd64 c-n-f Metadata [804 B]\n",
      "Get:35 http://mirror.yandex.ru/mirrors/postgresql focal-pgdg/main amd64 Packages [230 kB]\n",
      "Get:36 http://security.ubuntu.com/ubuntu focal-security/main i386 Packages [401 kB]\n",
      "Get:37 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1,316 kB]\n",
      "Get:38 https://packages.fluentbit.io/ubuntu/focal focal/main amd64 Packages [10.8 kB]\n",
      "Get:39 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [230 kB]\n",
      "Get:40 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [9,772 B]\n",
      "Get:41 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [799 kB]\n",
      "Get:42 http://security.ubuntu.com/ubuntu focal-security/restricted i386 Packages [21.7 kB]\n",
      "Get:43 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [114 kB]\n",
      "Get:44 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [504 B]\n",
      "Get:45 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [691 kB]\n",
      "Get:46 http://security.ubuntu.com/ubuntu focal-security/universe i386 Packages [546 kB]\n",
      "Get:47 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [120 kB]\n",
      "Get:48 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [14.0 kB]\n",
      "Get:49 http://security.ubuntu.com/ubuntu focal-security/multiverse i386 Packages [7,180 B]\n",
      "Get:50 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [20.7 kB]\n",
      "Get:51 http://security.ubuntu.com/ubuntu focal-security/multiverse Translation-en [5,196 B]\n",
      "Get:52 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 c-n-f Metadata [500 B]\n",
      "Fetched 10.5 MB in 2s (4,531 kB/s)                                \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fontconfig fonts-liberation libann0 libcairo2 libcdt5 libcgraph6 libdatrie1\n",
      "  libgd3 libgraphite2-3 libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libharfbuzz0b\n",
      "  libice6 libjbig0 liblab-gamut1 libpango-1.0-0 libpangocairo-1.0-0\n",
      "  libpangoft2-1.0-0 libpathplan4 libpixman-1-0 libsm6 libthai-data libthai0\n",
      "  libtiff5 libwebp6 libxaw7 libxcb-render0 libxcb-shm0 libxmu6 libxpm4 libxt6\n",
      "Suggested packages:\n",
      "  gsfonts graphviz-doc libgd-tools\n",
      "The following NEW packages will be installed:\n",
      "  fontconfig fonts-liberation graphviz libann0 libcairo2 libcdt5 libcgraph6\n",
      "  libdatrie1 libgd3 libgraphite2-3 libgts-0.7-5 libgts-bin libgvc6 libgvpr2\n",
      "  libharfbuzz0b libice6 libjbig0 liblab-gamut1 libpango-1.0-0\n",
      "  libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4 libpixman-1-0 libsm6\n",
      "  libthai-data libthai0 libtiff5 libwebp6 libxaw7 libxcb-render0 libxcb-shm0\n",
      "  libxmu6 libxpm4 libxt6\n",
      "0 upgraded, 34 newly installed, 0 to remove and 90 not upgraded.\n",
      "Need to get 5,522 kB of archives.\n",
      "After this operation, 19.6 MB of additional disk space will be used.\n",
      "Get:1 http://mirror.yandex.ru/ubuntu focal/main amd64 fontconfig amd64 2.13.1-2ubuntu3 [171 kB]\n",
      "Get:2 http://mirror.yandex.ru/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\n",
      "Get:3 http://mirror.yandex.ru/ubuntu focal/universe amd64 libann0 amd64 1.1.2+doc-7build1 [26.0 kB]\n",
      "Get:4 http://mirror.yandex.ru/ubuntu focal/universe amd64 libcdt5 amd64 2.42.2-3build2 [18.7 kB]\n",
      "Get:5 http://mirror.yandex.ru/ubuntu focal/universe amd64 libcgraph6 amd64 2.42.2-3build2 [41.3 kB]\n",
      "Get:6 http://mirror.yandex.ru/ubuntu focal/main amd64 libjbig0 amd64 2.1-3.1build1 [26.7 kB]\n",
      "Get:7 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 libwebp6 amd64 0.6.1-2ubuntu0.20.04.1 [185 kB]\n",
      "Get:8 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 libtiff5 amd64 4.1.0+git191117-2ubuntu0.20.04.2 [162 kB]\n",
      "Get:9 http://mirror.yandex.ru/ubuntu focal/main amd64 libxpm4 amd64 1:3.5.12-1 [34.0 kB]\n",
      "Get:10 http://mirror.yandex.ru/ubuntu focal-updates/main amd64 libgd3 amd64 2.2.5-5.2ubuntu2.1 [118 kB]\n",
      "Get:11 http://mirror.yandex.ru/ubuntu focal/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\n",
      "Get:12 http://mirror.yandex.ru/ubuntu focal/main amd64 libpixman-1-0 amd64 0.38.4-0ubuntu1 [227 kB]\n",
      "Get:13 http://mirror.yandex.ru/ubuntu focal/main amd64 libxcb-render0 amd64 1.14-2 [14.8 kB]\n",
      "Get:14 http://mirror.yandex.ru/ubuntu focal/main amd64 libxcb-shm0 amd64 1.14-2 [5,584 B]\n",
      "Get:15 http://mirror.yandex.ru/ubuntu focal/main amd64 libcairo2 amd64 1.16.0-4ubuntu1 [583 kB]\n",
      "Get:16 http://mirror.yandex.ru/ubuntu focal/main amd64 libgraphite2-3 amd64 1.3.13-11build1 [73.5 kB]\n",
      "Get:17 http://mirror.yandex.ru/ubuntu focal/main amd64 libharfbuzz0b amd64 2.6.4-1ubuntu4 [391 kB]\n",
      "Get:18 http://mirror.yandex.ru/ubuntu focal/main amd64 libthai-data all 0.1.28-3 [134 kB]\n",
      "Get:19 http://mirror.yandex.ru/ubuntu focal/main amd64 libdatrie1 amd64 0.2.12-3 [18.7 kB]\n",
      "Get:20 http://mirror.yandex.ru/ubuntu focal/main amd64 libthai0 amd64 0.1.28-3 [18.1 kB]\n",
      "Get:21 http://mirror.yandex.ru/ubuntu focal/main amd64 libpango-1.0-0 amd64 1.44.7-2ubuntu4 [162 kB]\n",
      "Get:22 http://mirror.yandex.ru/ubuntu focal/main amd64 libpangoft2-1.0-0 amd64 1.44.7-2ubuntu4 [34.9 kB]\n",
      "Get:23 http://mirror.yandex.ru/ubuntu focal/main amd64 libpangocairo-1.0-0 amd64 1.44.7-2ubuntu4 [24.8 kB]\n",
      "Get:24 http://mirror.yandex.ru/ubuntu focal/universe amd64 libpathplan4 amd64 2.42.2-3build2 [21.9 kB]\n",
      "Get:25 http://mirror.yandex.ru/ubuntu focal/universe amd64 libgvc6 amd64 2.42.2-3build2 [647 kB]\n",
      "Get:26 http://mirror.yandex.ru/ubuntu focal/universe amd64 libgvpr2 amd64 2.42.2-3build2 [167 kB]\n",
      "Get:27 http://mirror.yandex.ru/ubuntu focal/universe amd64 liblab-gamut1 amd64 2.42.2-3build2 [177 kB]\n",
      "Get:28 http://mirror.yandex.ru/ubuntu focal/main amd64 libice6 amd64 2:1.0.10-0ubuntu1 [41.0 kB]\n",
      "Get:29 http://mirror.yandex.ru/ubuntu focal/main amd64 libsm6 amd64 2:1.2.3-1 [16.1 kB]\n",
      "Get:30 http://mirror.yandex.ru/ubuntu focal/main amd64 libxt6 amd64 1:1.1.5-1 [160 kB]\n",
      "Get:31 http://mirror.yandex.ru/ubuntu focal/main amd64 libxmu6 amd64 2:1.1.3-0ubuntu1 [45.8 kB]\n",
      "Get:32 http://mirror.yandex.ru/ubuntu focal/main amd64 libxaw7 amd64 2:1.0.13-1 [173 kB]\n",
      "Get:33 http://mirror.yandex.ru/ubuntu focal/universe amd64 graphviz amd64 2.42.2-3build2 [590 kB]\n",
      "Get:34 http://mirror.yandex.ru/ubuntu focal/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\n",
      "Fetched 5,522 kB in 1s (10.2 MB/s)      \n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package fontconfig.\n",
      "(Reading database ... 89886 files and directories currently installed.)\n",
      "Preparing to unpack .../00-fontconfig_2.13.1-2ubuntu3_amd64.deb ...\n",
      "Unpacking fontconfig (2.13.1-2ubuntu3) ...\n",
      "Selecting previously unselected package fonts-liberation.\n",
      "Preparing to unpack .../01-fonts-liberation_1%3a1.07.4-11_all.deb ...\n",
      "Unpacking fonts-liberation (1:1.07.4-11) ...\n",
      "Selecting previously unselected package libann0.\n",
      "Preparing to unpack .../02-libann0_1.1.2+doc-7build1_amd64.deb ...\n",
      "Unpacking libann0 (1.1.2+doc-7build1) ...\n",
      "Selecting previously unselected package libcdt5:amd64.\n",
      "Preparing to unpack .../03-libcdt5_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking libcdt5:amd64 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libcgraph6:amd64.\n",
      "Preparing to unpack .../04-libcgraph6_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libjbig0:amd64.\n",
      "Preparing to unpack .../05-libjbig0_2.1-3.1build1_amd64.deb ...\n",
      "Unpacking libjbig0:amd64 (2.1-3.1build1) ...\n",
      "Selecting previously unselected package libwebp6:amd64.\n",
      "Preparing to unpack .../06-libwebp6_0.6.1-2ubuntu0.20.04.1_amd64.deb ...\n",
      "Unpacking libwebp6:amd64 (0.6.1-2ubuntu0.20.04.1) ...\n",
      "Selecting previously unselected package libtiff5:amd64.\n",
      "Preparing to unpack .../07-libtiff5_4.1.0+git191117-2ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libtiff5:amd64 (4.1.0+git191117-2ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libxpm4:amd64.\n",
      "Preparing to unpack .../08-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
      "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
      "Selecting previously unselected package libgd3:amd64.\n",
      "Preparing to unpack .../09-libgd3_2.2.5-5.2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgd3:amd64 (2.2.5-5.2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgts-0.7-5:amd64.\n",
      "Preparing to unpack .../10-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
      "Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "Selecting previously unselected package libpixman-1-0:amd64.\n",
      "Preparing to unpack .../11-libpixman-1-0_0.38.4-0ubuntu1_amd64.deb ...\n",
      "Unpacking libpixman-1-0:amd64 (0.38.4-0ubuntu1) ...\n",
      "Selecting previously unselected package libxcb-render0:amd64.\n",
      "Preparing to unpack .../12-libxcb-render0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-render0:amd64 (1.14-2) ...\n",
      "Selecting previously unselected package libxcb-shm0:amd64.\n",
      "Preparing to unpack .../13-libxcb-shm0_1.14-2_amd64.deb ...\n",
      "Unpacking libxcb-shm0:amd64 (1.14-2) ...\n",
      "Selecting previously unselected package libcairo2:amd64.\n",
      "Preparing to unpack .../14-libcairo2_1.16.0-4ubuntu1_amd64.deb ...\n",
      "Unpacking libcairo2:amd64 (1.16.0-4ubuntu1) ...\n",
      "Selecting previously unselected package libgraphite2-3:amd64.\n",
      "Preparing to unpack .../15-libgraphite2-3_1.3.13-11build1_amd64.deb ...\n",
      "Unpacking libgraphite2-3:amd64 (1.3.13-11build1) ...\n",
      "Selecting previously unselected package libharfbuzz0b:amd64.\n",
      "Preparing to unpack .../16-libharfbuzz0b_2.6.4-1ubuntu4_amd64.deb ...\n",
      "Unpacking libharfbuzz0b:amd64 (2.6.4-1ubuntu4) ...\n",
      "Selecting previously unselected package libthai-data.\n",
      "Preparing to unpack .../17-libthai-data_0.1.28-3_all.deb ...\n",
      "Unpacking libthai-data (0.1.28-3) ...\n",
      "Selecting previously unselected package libdatrie1:amd64.\n",
      "Preparing to unpack .../18-libdatrie1_0.2.12-3_amd64.deb ...\n",
      "Unpacking libdatrie1:amd64 (0.2.12-3) ...\n",
      "Selecting previously unselected package libthai0:amd64.\n",
      "Preparing to unpack .../19-libthai0_0.1.28-3_amd64.deb ...\n",
      "Unpacking libthai0:amd64 (0.1.28-3) ...\n",
      "Selecting previously unselected package libpango-1.0-0:amd64.\n",
      "Preparing to unpack .../20-libpango-1.0-0_1.44.7-2ubuntu4_amd64.deb ...\n",
      "Unpacking libpango-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
      "Preparing to unpack .../21-libpangoft2-1.0-0_1.44.7-2ubuntu4_amd64.deb ...\n",
      "Unpacking libpangoft2-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
      "Preparing to unpack .../22-libpangocairo-1.0-0_1.44.7-2ubuntu4_amd64.deb ...\n",
      "Unpacking libpangocairo-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Selecting previously unselected package libpathplan4:amd64.\n",
      "Preparing to unpack .../23-libpathplan4_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libgvc6.\n",
      "Preparing to unpack .../24-libgvc6_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking libgvc6 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libgvpr2:amd64.\n",
      "Preparing to unpack .../25-libgvpr2_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package liblab-gamut1:amd64.\n",
      "Preparing to unpack .../26-liblab-gamut1_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../27-libice6_2%3a1.0.10-0ubuntu1_amd64.deb ...\n",
      "Unpacking libice6:amd64 (2:1.0.10-0ubuntu1) ...\n",
      "Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../28-libsm6_2%3a1.2.3-1_amd64.deb ...\n",
      "Unpacking libsm6:amd64 (2:1.2.3-1) ...\n",
      "Selecting previously unselected package libxt6:amd64.\n",
      "Preparing to unpack .../29-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
      "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
      "Selecting previously unselected package libxmu6:amd64.\n",
      "Preparing to unpack .../30-libxmu6_2%3a1.1.3-0ubuntu1_amd64.deb ...\n",
      "Unpacking libxmu6:amd64 (2:1.1.3-0ubuntu1) ...\n",
      "Selecting previously unselected package libxaw7:amd64.\n",
      "Preparing to unpack .../31-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
      "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
      "Selecting previously unselected package graphviz.\n",
      "Preparing to unpack .../32-graphviz_2.42.2-3build2_amd64.deb ...\n",
      "Unpacking graphviz (2.42.2-3build2) ...\n",
      "Selecting previously unselected package libgts-bin.\n",
      "Preparing to unpack .../33-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
      "Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
      "Setting up libgraphite2-3:amd64 (1.3.13-11build1) ...\n",
      "Setting up libpixman-1-0:amd64 (0.38.4-0ubuntu1) ...\n",
      "Setting up libice6:amd64 (2:1.0.10-0ubuntu1) ...\n",
      "Setting up fontconfig (2.13.1-2ubuntu3) ...\n",
      "Regenerating fonts cache... done.\n",
      "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
      "Setting up libdatrie1:amd64 (0.2.12-3) ...\n",
      "Setting up libxcb-render0:amd64 (1.14-2) ...\n",
      "Setting up liblab-gamut1:amd64 (2.42.2-3build2) ...\n",
      "Setting up libxcb-shm0:amd64 (1.14-2) ...\n",
      "Setting up libjbig0:amd64 (2.1-3.1build1) ...\n",
      "Setting up libcairo2:amd64 (1.16.0-4ubuntu1) ...\n",
      "Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
      "Setting up libpathplan4:amd64 (2.42.2-3build2) ...\n",
      "Setting up libann0 (1.1.2+doc-7build1) ...\n",
      "Setting up libwebp6:amd64 (0.6.1-2ubuntu0.20.04.1) ...\n",
      "Setting up fonts-liberation (1:1.07.4-11) ...\n",
      "Setting up libharfbuzz0b:amd64 (2.6.4-1ubuntu4) ...\n",
      "Setting up libthai-data (0.1.28-3) ...\n",
      "Setting up libcdt5:amd64 (2.42.2-3build2) ...\n",
      "Setting up libcgraph6:amd64 (2.42.2-3build2) ...\n",
      "Setting up libtiff5:amd64 (4.1.0+git191117-2ubuntu0.20.04.2) ...\n",
      "Setting up libsm6:amd64 (2:1.2.3-1) ...\n",
      "Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
      "Setting up libthai0:amd64 (0.1.28-3) ...\n",
      "Setting up libgd3:amd64 (2.2.5-5.2ubuntu2.1) ...\n",
      "Setting up libxt6:amd64 (1:1.1.5-1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libgvpr2:amd64 (2.42.2-3build2) ...\n",
      "Setting up libxmu6:amd64 (2:1.1.3-0ubuntu1) ...\n",
      "Setting up libpango-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
      "Setting up libpangoft2-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Setting up libpangocairo-1.0-0:amd64 (1.44.7-2ubuntu4) ...\n",
      "Setting up libgvc6 (2.42.2-3build2) ...\n",
      "Setting up graphviz (2.42.2-3build2) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.2) ...\n",
      "\u001b[33m  WARNING: The script dateparser-download is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts jupyter, jupyter-migrate and jupyter-troubleshoot are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jupyter-trust is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script send2trash is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script pygmentize is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts ipython and ipython3 are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jupyter-execute is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts jupyter-dejavu and jupyter-nbconvert are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts jupyter-bundlerextension, jupyter-nbextension, jupyter-notebook and jupyter-serverextension are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get update -y && sudo apt-get install graphviz -y\n",
    "! pip install -q numpy pandas sklearn dateparser pandarallel ipywidgets catboost graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! /opt/conda/bin/jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "driver:\r\n",
      "  --onethread           Disable parse thread\r\n",
      "VW options:\r\n",
      "  --ring_size arg (=256, ) size of example ring\r\n",
      "  --strict_parse           throw on malformed examples\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg Set learning rate\r\n",
      "  --power_t arg              t power value\r\n",
      "  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\r\n",
      "  --initial_t arg            initial t value\r\n"
     ]
    }
   ],
   "source": [
    "! vw --help | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dateparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174767397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15961515"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 |sq      |t  |pos position:7     |ot objectType:3 |h histCTR:0.000485    |pr price:1099.0 |ic isContext:1     |cnt count:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_vw(train_df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.convert_to_vw(row)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.udf.register(\"convert_to_vw\", convert_to_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vw(rdd_, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for row_ in rdd_:\n",
    "            f.write(convert_to_vw(row_) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                        (5 + 5) / 10]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41019)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 596, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o60.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41019)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 596, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o60.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41019)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 596, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o60.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41019)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 596, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o60.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:41019)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 596, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o60.collectToPython\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# java.lang.OutOfMemoryError: Java heap space\n",
      "# -XX:OnOutOfMemoryError=\"kill -9 %p\"\n",
      "#   Executing /bin/sh -c \"kill -9 7826\"...\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:41019)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:596\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 596\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o60.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:977\u001b[0m, in \u001b[0;36mGatewayClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeque\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1115\u001b[0m, in \u001b[0;36mGatewayConnection.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:138\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    139\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    141\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:596\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m>>> df.collect()\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m[Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 596\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/traceback_utils.py:78\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     76\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1031\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:979\u001b[0m, in \u001b[0;36mGatewayClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeque\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 979\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:985\u001b[0m, in \u001b[0;36mGatewayClient._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    983\u001b[0m     connection \u001b[38;5;241m=\u001b[39m GatewayConnection(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property)\n\u001b[0;32m--> 985\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1127\u001b[0m, in \u001b[0;36mGatewayConnection.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while trying to connect to the Java \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m   1126\u001b[0m logger\u001b[38;5;241m.\u001b[39mexception(msg)\n\u001b[0;32m-> 1127\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(msg, e)\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:41019)"
     ]
    }
   ],
   "source": [
    "train_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_df.rdd.collect()\n",
    "test_rdd = test_df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(SearchQuery='отдам', Position=7, ObjectType=3, HistCTR=0.000485, Price=1099.0, Title='Платье и леггинсы', IsContext=1, count=0, IsClick=0),\n",
       " Row(SearchQuery='отдам', Position=1, ObjectType=3, HistCTR=1e-05, Price=1099.0, Title='Платье и леггинсы', IsContext=1, count=0, IsClick=0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_vw(train_df, 'avito.train.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 ubuntu hadoop 5350670676 2022-02-20 22:30 /user/avito/AdsInfo.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/avito/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/spark/top10 || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 18:55:04,553 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 11.0 (TID 39, rc1a-dataproc-c-dh5md77ip9o1c50g.mdb.yandexcloud.net, executor 1): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'IsClick' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n",
      "    for x in iterator:\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000002/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: IsClick\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n",
      "\t... 9 more\n",
      "\n",
      "2022-03-12 18:55:05,214 ERROR scheduler.TaskSetManager: Task 1 in stage 11.0 failed 4 times; aborting job\n",
      "2022-03-12 18:55:05,231 ERROR io.SparkHadoopWriter: Aborting job job_202203121855036951094268908714824_0052.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 4 times, most recent failure: Lost task 1.3 in stage 11.0 (TID 53, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 3): org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'IsClick' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n",
      "    for x in iterator:\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: IsClick\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n",
      "\t... 9 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2167)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'IsClick' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n",
      "    for x in iterator:\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n",
      "  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: IsClick\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n",
      "\t... 9 more\n",
      "2022-03-12 18:55:05,274 WARN scheduler.TaskSetManager: Lost task 4.2 in stage 11.0 (TID 56, rc1a-dataproc-c-dh5md77ip9o1c50g.mdb.yandexcloud.net, executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-12 18:55:05,330 WARN scheduler.TaskSetManager: Lost task 0.3 in stage 11.0 (TID 52, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-12 18:55:05,355 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 11.0 (TID 58, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-12 18:55:05,370 WARN scheduler.TaskSetManager: Lost task 3.2 in stage 11.0 (TID 57, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 5): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:105)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 4 times, most recent failure: Lost task 1.3 in stage 11.0 (TID 53, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 3): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'IsClick' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n    for x in iterator:\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n    raise ValueError(item)\nValueError: IsClick\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2167)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 50 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'IsClick' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n    for x in iterator:\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n    raise ValueError(item)\nValueError: IsClick\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n\t... 9 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwrite_vw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavito.test.vw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mwrite_vw\u001b[0;34m(dataframe, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_vw\u001b[39m(dataframe, filename):\n\u001b[1;32m      2\u001b[0m     (\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mdataframe\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_vw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:1656\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     keyed\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mBytesToString())\u001b[38;5;241m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1656\u001b[0m     \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    130\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:105)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 4 times, most recent failure: Lost task 1.3 in stage 11.0 (TID 53, rc1a-dataproc-d-n71egq5kd327byup.mdb.yandexcloud.net, executor 3): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'IsClick' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n    for x in iterator:\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n    raise ValueError(item)\nValueError: IsClick\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2167)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 50 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:162)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1582, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'IsClick' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1644, in func\n    for x in iterator:\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_2231/3853469752.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_2231/4044605215.py\", line 8, in convert_to_vw\n  File \"/hadoop/yarn/nm-local-dir/usercache/ubuntu/appcache/application_1647109525436_0001/container_1647109525436_0001_01_000004/pyspark.zip/pyspark/sql/types.py\", line 1587, in __getitem__\n    raise ValueError(item)\nValueError: IsClick\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:135)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:134)\n\t... 9 more\n"
     ]
    }
   ],
   "source": [
    "write_vw(test_df, 'avito.test.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "**2. [+5 баллов]** Разделите ваш датасет на обучающую и тестовую выборку. Обучите **логистическую регрессию** с помощью **Vowpal Wabbit**. \n",
    "\n",
    "Сделайте бинарные предсказания и посчитайте Accuracy, Presicion и Recall на тестовой выборке. \n",
    "Сделайте предсказания вероятностей и посчитайте LogLoss. (Эта же метрика используется в Kaggle для оценивания вашего решения).\n",
    "\n",
    "Посчитайте ответы для тестовой выборки, которая предлагается в задании и отправьте в Kaggle (само соревнование уже давно закончилось, но вы все еще можете присылать туда свои решения и система будет вас оценивать). В решении приложите скриншот того, какой скор вам выдал Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'pyspark.sql.dataframe.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-6521ea696ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(train_df, test_size=0.2, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. [+ up to 5 дополнительных баллов]** Несмотря на то, что мы просим вас считать качество (и на тестовой и в Kaggle), оно никак **не будет влиять** на основные баллы. Основная цель этого задания состоит в том, чтобы вы потренировались в End-to-end работе с ML задачей на больших данных - от сбора датасета до получения модели и ее оценки. \n",
    "\n",
    "Однако, топ 5 студентов (в каждой группе отдельно) с самым больших качеством (по Kaggle) получат до 5 дополнительных баллов в зависимости от полученой метрики качества (за самое лучшее качество +5, за второе место +4 и так далее).\n",
    "\n",
    "**Важно** В сдаваемом ноубуке должны присутствовать такие элементы\n",
    "* Код на Spark, который собирает датасет\n",
    "* Команды запуска VW для его обучения\n",
    "* Подсчитанные метрики качества (и соответственно код, который их считает)\n",
    "* Скриншот из Kaggle, на котором видно вашу посылку и подсчитанную метрику качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

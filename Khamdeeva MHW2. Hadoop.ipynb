{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Мини задание 2\n",
    "\n",
    "В этой домашней работе будет два подзадания.\n",
    "\n",
    "Мы продолжает работать с датасетом из Авито - https://www.kaggle.com/c/avito-context-ad-clicks .\n",
    "\n",
    "**1. [+5 баллов]** В `VisitsStream.tsv` лежит информация про пользователей, которые открывают сайт. Используя классический Hadoop MapReduce необходимо посчитать топ 10 пользователей с самыми длинными по времени сессиями и время этой самой длинной сессии (в секундах).\n",
    "\n",
    "Сессия определяется следующим образом - это окно времени, внутри которого временное расстояние от двух соседних посещений не более **15 минут**. \n",
    "\n",
    "Иными словами - если пользователь зашел на сайт в момент X и последнее предыдущее посещение сайта в момент Y было не позднее чем 15 минут назад, то сессия \"продлевается\" до текущего момента. Если же временное расстояние от X до Y более 15 минут, то считается, что предыдущая сессия закончилась в момент Y, а новая сессия началась в момент X.\n",
    "\n",
    "Сессия может длится 0 секунд, если пользователь сделал всего 1 запрос в течение 30 минутного окна (в середине этого окна). Считается, что в начале у пользователя нет открытой сессии и что сессия автоматически заканчивается, когда записей больше не осталось.\n",
    "\n",
    "Выводить нужно только уникальных пользователей и для каждого такого пользователя находить время самой длинной его сессии. \n",
    "\n",
    "При решении можно использовать произвольное количество MapReduce задач, но чем меньше, тем лучше. За излишне неоптимальное решение можно потерять балл. \n",
    "\n",
    "Полученный файл с топ 10 нужно будет выложить в облако, обеспечить публичный доступ до него и приложить к решению.\n",
    "\n",
    "В ноутбуке должны присутствовать ячейки с \n",
    "\n",
    "1) Всеми необходимыми скриптами для работы ваших MapReduce задач\n",
    "\n",
    "2) Командами запуска самих MapReduce задач\n",
    "\n",
    "3) Ссылкой на итоговый результат работы в вашем облаке. Ссылки должны быть рабочими до того момента, как вашу домашку не проверят.\n",
    "\n",
    "Пример итогового файла\n",
    "\n",
    "```bash\n",
    "1000094\t6852\n",
    "1000030\t4237\n",
    "1000003\t1932\n",
    "1000058\t1885\n",
    "100010\t1132\n",
    "1000012\t1086\n",
    "1000067\t657\n",
    "1000111\t244\n",
    "1000085\t197\n",
    "1000049\t131\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID\tIPID\tAdID\tViewDate\r\n",
      "59703\t1259356\t469877\t2015-04-25 00:00:00.0\r\n",
      "154389\t1846749\t27252551\t2015-04-25 00:00:00.0\r\n",
      "218628\t2108380\t31685325\t2015-04-25 00:00:00.0\r\n",
      "231535\t837110\t18827716\t2015-04-25 00:00:00.0\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 /home/ubuntu/mhw1/VisitsStream.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем голову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd /home/ubuntu/mhw1/VisitsStream.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кладем в hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/visits/data\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/visits/data\n",
    "! hdfs dfs -mkdir -p /user/visits/data\n",
    "! hdfs dfs -put /home/ubuntu/mhw1/VisitsStream.tsv /user/visits/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59703\t1259356\t469877\t2015-04-25 00:00:00.0\r\n",
      "154389\t1846749\t27252551\t2015-04-25 00:00:00.0\r\n",
      "218628\t2108380\t31685325\t2015-04-25 00:00:00.0\r\n",
      "231535\t837110\t18827716\t2015-04-25 00:00:00.0\r\n",
      "282306\t1654210\t29363673\t2015-04-25 00:00:00.0\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 /home/ubuntu/mhw1/VisitsStream.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! sudo hdfs balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:09:38,819 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020]\n",
      "2022-02-21 11:09:38,823 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2022-02-21 11:09:38,824 INFO balancer.Balancer: included nodes = []\n",
      "2022-02-21 11:09:38,824 INFO balancer.Balancer: excluded nodes = []\n",
      "2022-02-21 11:09:38,824 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2022-02-21 11:09:38,827 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2022-02-21 11:09:40,051 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2022-02-21 11:09:40,058 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2022-02-21 11:09:40,058 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2022-02-21 11:09:40,078 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.35:9866\n",
      "2022-02-21 11:09:40,080 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2022-02-21 11:09:40,080 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Feb 21, 2022 11:09:40 AM          0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8020\n",
      "Feb 21, 2022 11:09:40 AM Balancing took 1.857 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo chmod 0777 /usr/lib/hadoop/logs\n",
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop 13180996366 2022-02-20 14:12 /user/visits/data/VisitsStream.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/visits/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Longest session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting longest-session.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest-session.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime(1970,1,1)\n",
    "delta_session = 900.0\n",
    "\n",
    "# конвертируем дату в секунды\n",
    "def date2seconds(date_iso_str):\n",
    "    cur_date = datetime.strptime(date_iso_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    delta = cur_date - start_date\n",
    "    return delta.total_seconds()\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in csv.reader(iter(sys.stdin.readline, ''),  delimiter=\"\\t\"):\n",
    "        user, date = row[0], row[3]\n",
    "        seconds = date2seconds(date)\n",
    "        print(\"{}\\t{}\".format(user, seconds))\n",
    "\n",
    "def reducer():\n",
    "    user, enter = next(sys.stdin).split('\\t')\n",
    "    enter = float(enter)\n",
    "    \n",
    "    max_duration = cur_duration = 0.0\n",
    "    last_enter = enter\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        current_user, current_enter = line.split('\\t')\n",
    "        current_enter =  float(current_enter)\n",
    "        \n",
    "        if current_user != user:\n",
    "            max_duration = max(max_duration, cur_duration)\n",
    "            print(\"{}\\t{}\".format(user, max_duration))\n",
    "            \n",
    "            user = current_user\n",
    "            enter = current_enter\n",
    "            \n",
    "            max_duration = cur_duration = 0.0\n",
    "        else:\n",
    "            if current_enter - last_enter <= delta_session:\n",
    "                cur_duration += current_enter - last_enter \n",
    "            else:\n",
    "                max_duration = max(max_duration, cur_duration)\n",
    "                cur_duration = 0.0\n",
    "        last_enter = current_enter\n",
    "                \n",
    "                \n",
    "    max_duration = max(max_duration, cur_duration)      \n",
    "    print(\"{}\\t{}\".format(user, max_duration))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/visits/result\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob3761870572196447539.jar tmpDir=null\n",
      "2022-02-20 15:10:42,072 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:8032\n",
      "2022-02-20 15:10:42,307 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:10200\n",
      "2022-02-20 15:10:42,350 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:8032\n",
      "2022-02-20 15:10:42,351 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:10200\n",
      "2022-02-20 15:10:42,590 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1645357742880_0002\n",
      "2022-02-20 15:10:43,297 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-02-20 15:10:43,771 INFO mapreduce.JobSubmitter: number of splits:50\n",
      "2022-02-20 15:10:44,327 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1645357742880_0002\n",
      "2022-02-20 15:10:44,328 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-02-20 15:10:44,502 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-02-20 15:10:44,502 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-02-20 15:10:44,611 INFO impl.YarnClientImpl: Submitted application application_1645357742880_0002\n",
      "2022-02-20 15:10:44,720 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8088/proxy/application_1645357742880_0002/\n",
      "2022-02-20 15:10:44,722 INFO mapreduce.Job: Running job: job_1645357742880_0002\n",
      "2022-02-20 15:10:51,855 INFO mapreduce.Job: Job job_1645357742880_0002 running in uber mode : false\n",
      "2022-02-20 15:10:51,856 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-02-20 15:11:11,142 INFO mapreduce.Job:  map 1% reduce 0%\n",
      "2022-02-20 15:11:28,464 INFO mapreduce.Job:  map 2% reduce 0%\n",
      "2022-02-20 15:11:41,657 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2022-02-20 15:12:05,154 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "2022-02-20 15:12:23,275 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-02-20 15:12:30,339 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "2022-02-20 15:12:52,562 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2022-02-20 15:13:10,945 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2022-02-20 15:13:23,161 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2022-02-20 15:13:26,182 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2022-02-20 15:13:46,541 INFO mapreduce.Job:  map 12% reduce 0%\n",
      "2022-02-20 15:14:04,697 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2022-02-20 15:14:12,799 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "2022-02-20 15:14:35,940 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "2022-02-20 15:14:54,030 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "2022-02-20 15:15:12,362 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2022-02-20 15:15:30,439 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "2022-02-20 15:15:48,810 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2022-02-20 15:15:54,837 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2022-02-20 15:15:55,841 INFO mapreduce.Job:  map 21% reduce 0%\n",
      "2022-02-20 15:15:58,856 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2022-02-20 15:16:15,050 INFO mapreduce.Job:  map 22% reduce 1%\n",
      "2022-02-20 15:16:18,061 INFO mapreduce.Job:  map 23% reduce 1%\n",
      "2022-02-20 15:16:40,139 INFO mapreduce.Job:  map 24% reduce 1%\n",
      "2022-02-20 15:17:10,245 INFO mapreduce.Job:  map 25% reduce 1%\n",
      "2022-02-20 15:17:32,327 INFO mapreduce.Job:  map 26% reduce 1%\n",
      "2022-02-20 15:18:00,436 INFO mapreduce.Job:  map 27% reduce 1%\n",
      "2022-02-20 15:18:20,513 INFO mapreduce.Job:  map 28% reduce 1%\n",
      "2022-02-20 15:18:28,543 INFO mapreduce.Job:  map 30% reduce 1%\n",
      "2022-02-20 15:18:47,611 INFO mapreduce.Job:  map 31% reduce 1%\n",
      "2022-02-20 15:19:10,690 INFO mapreduce.Job:  map 32% reduce 1%\n",
      "2022-02-20 15:19:40,800 INFO mapreduce.Job:  map 33% reduce 1%\n",
      "2022-02-20 15:20:04,884 INFO mapreduce.Job:  map 34% reduce 1%\n",
      "2022-02-20 15:20:30,981 INFO mapreduce.Job:  map 35% reduce 1%\n",
      "2022-02-20 15:20:53,053 INFO mapreduce.Job:  map 36% reduce 1%\n",
      "2022-02-20 15:20:58,075 INFO mapreduce.Job:  map 37% reduce 1%\n",
      "2022-02-20 15:21:00,081 INFO mapreduce.Job:  map 38% reduce 1%\n",
      "2022-02-20 15:21:19,179 INFO mapreduce.Job:  map 39% reduce 1%\n",
      "2022-02-20 15:21:40,246 INFO mapreduce.Job:  map 40% reduce 1%\n",
      "2022-02-20 15:22:08,333 INFO mapreduce.Job:  map 41% reduce 1%\n",
      "2022-02-20 15:22:32,422 INFO mapreduce.Job:  map 42% reduce 1%\n",
      "2022-02-20 15:22:59,517 INFO mapreduce.Job:  map 43% reduce 1%\n",
      "2022-02-20 15:23:20,584 INFO mapreduce.Job:  map 44% reduce 1%\n",
      "2022-02-20 15:23:26,602 INFO mapreduce.Job:  map 45% reduce 1%\n",
      "2022-02-20 15:23:32,620 INFO mapreduce.Job:  map 46% reduce 1%\n",
      "2022-02-20 15:23:33,623 INFO mapreduce.Job:  map 46% reduce 2%\n",
      "2022-02-20 15:23:43,653 INFO mapreduce.Job:  map 46% reduce 3%\n",
      "2022-02-20 15:23:48,668 INFO mapreduce.Job:  map 47% reduce 3%\n",
      "2022-02-20 15:24:12,740 INFO mapreduce.Job:  map 48% reduce 3%\n",
      "2022-02-20 15:24:39,847 INFO mapreduce.Job:  map 49% reduce 3%\n",
      "2022-02-20 15:25:03,918 INFO mapreduce.Job:  map 50% reduce 3%\n",
      "2022-02-20 15:25:14,953 INFO mapreduce.Job:  map 51% reduce 3%\n",
      "2022-02-20 15:25:24,984 INFO mapreduce.Job:  map 52% reduce 3%\n",
      "2022-02-20 15:25:43,043 INFO mapreduce.Job:  map 53% reduce 3%\n",
      "2022-02-20 15:26:02,106 INFO mapreduce.Job:  map 54% reduce 3%\n",
      "2022-02-20 15:26:31,198 INFO mapreduce.Job:  map 55% reduce 3%\n",
      "2022-02-20 15:26:55,274 INFO mapreduce.Job:  map 56% reduce 3%\n",
      "2022-02-20 15:27:07,308 INFO mapreduce.Job:  map 57% reduce 3%\n",
      "2022-02-20 15:27:08,312 INFO mapreduce.Job:  map 57% reduce 4%\n",
      "2022-02-20 15:27:18,345 INFO mapreduce.Job:  map 58% reduce 4%\n",
      "2022-02-20 15:27:34,392 INFO mapreduce.Job:  map 59% reduce 4%\n",
      "2022-02-20 15:27:57,464 INFO mapreduce.Job:  map 60% reduce 4%\n",
      "2022-02-20 15:28:21,537 INFO mapreduce.Job:  map 61% reduce 4%\n",
      "2022-02-20 15:28:46,611 INFO mapreduce.Job:  map 62% reduce 4%\n",
      "2022-02-20 15:28:58,651 INFO mapreduce.Job:  map 63% reduce 4%\n",
      "2022-02-20 15:29:11,701 INFO mapreduce.Job:  map 64% reduce 4%\n",
      "2022-02-20 15:29:23,761 INFO mapreduce.Job:  map 65% reduce 4%\n",
      "2022-02-20 15:29:49,839 INFO mapreduce.Job:  map 66% reduce 4%\n",
      "2022-02-20 15:30:13,913 INFO mapreduce.Job:  map 67% reduce 4%\n",
      "2022-02-20 15:30:36,977 INFO mapreduce.Job:  map 68% reduce 4%\n",
      "2022-02-20 15:30:51,015 INFO mapreduce.Job:  map 69% reduce 4%\n",
      "2022-02-20 15:30:58,035 INFO mapreduce.Job:  map 69% reduce 5%\n",
      "2022-02-20 15:31:04,052 INFO mapreduce.Job:  map 70% reduce 5%\n",
      "2022-02-20 15:31:13,083 INFO mapreduce.Job:  map 71% reduce 5%\n",
      "2022-02-20 15:31:41,163 INFO mapreduce.Job:  map 72% reduce 5%\n",
      "2022-02-20 15:32:05,229 INFO mapreduce.Job:  map 73% reduce 5%\n",
      "2022-02-20 15:32:29,316 INFO mapreduce.Job:  map 74% reduce 5%\n",
      "2022-02-20 15:32:42,352 INFO mapreduce.Job:  map 75% reduce 5%\n",
      "2022-02-20 15:32:53,382 INFO mapreduce.Job:  map 76% reduce 5%\n",
      "2022-02-20 15:33:08,426 INFO mapreduce.Job:  map 77% reduce 5%\n",
      "2022-02-20 15:33:32,495 INFO mapreduce.Job:  map 78% reduce 5%\n",
      "2022-02-20 15:33:57,574 INFO mapreduce.Job:  map 79% reduce 5%\n",
      "2022-02-20 15:34:21,640 INFO mapreduce.Job:  map 80% reduce 5%\n",
      "2022-02-20 15:34:33,678 INFO mapreduce.Job:  map 81% reduce 5%\n",
      "2022-02-20 15:34:48,722 INFO mapreduce.Job:  map 82% reduce 5%\n",
      "2022-02-20 15:35:00,757 INFO mapreduce.Job:  map 83% reduce 5%\n",
      "2022-02-20 15:35:24,825 INFO mapreduce.Job:  map 84% reduce 5%\n",
      "2022-02-20 15:35:51,902 INFO mapreduce.Job:  map 85% reduce 5%\n",
      "2022-02-20 15:36:15,982 INFO mapreduce.Job:  map 86% reduce 5%\n",
      "2022-02-20 15:36:28,024 INFO mapreduce.Job:  map 87% reduce 5%\n",
      "2022-02-20 15:36:29,027 INFO mapreduce.Job:  map 87% reduce 6%\n",
      "2022-02-20 15:36:41,063 INFO mapreduce.Job:  map 88% reduce 6%\n",
      "2022-02-20 15:36:52,093 INFO mapreduce.Job:  map 89% reduce 6%\n",
      "2022-02-20 15:37:16,160 INFO mapreduce.Job:  map 90% reduce 6%\n",
      "2022-02-20 15:37:41,229 INFO mapreduce.Job:  map 91% reduce 6%\n",
      "2022-02-20 15:38:06,301 INFO mapreduce.Job:  map 92% reduce 6%\n",
      "2022-02-20 15:38:16,330 INFO mapreduce.Job:  map 93% reduce 6%\n",
      "2022-02-20 15:38:33,380 INFO mapreduce.Job:  map 94% reduce 6%\n",
      "2022-02-20 15:38:42,407 INFO mapreduce.Job:  map 95% reduce 6%\n",
      "2022-02-20 15:38:53,449 INFO mapreduce.Job:  map 97% reduce 6%\n",
      "2022-02-20 15:39:08,494 INFO mapreduce.Job:  map 97% reduce 10%\n",
      "2022-02-20 15:39:15,515 INFO mapreduce.Job:  map 98% reduce 10%\n",
      "2022-02-20 15:39:39,587 INFO mapreduce.Job:  map 99% reduce 10%\n",
      "2022-02-20 15:39:47,624 INFO mapreduce.Job:  map 100% reduce 10%\n",
      "2022-02-20 15:39:53,643 INFO mapreduce.Job:  map 100% reduce 11%\n",
      "2022-02-20 15:39:56,651 INFO mapreduce.Job:  map 100% reduce 12%\n",
      "2022-02-20 15:39:59,662 INFO mapreduce.Job:  map 100% reduce 13%\n",
      "2022-02-20 15:40:02,671 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "2022-02-20 15:40:03,674 INFO mapreduce.Job:  map 100% reduce 18%\n",
      "2022-02-20 15:40:04,677 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "2022-02-20 15:40:05,681 INFO mapreduce.Job:  map 100% reduce 24%\n",
      "2022-02-20 15:40:09,694 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2022-02-20 15:40:11,707 INFO mapreduce.Job:  map 100% reduce 27%\n",
      "2022-02-20 15:40:15,757 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "2022-02-20 15:40:16,800 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2022-02-20 15:40:17,810 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2022-02-20 15:40:20,828 INFO mapreduce.Job:  map 100% reduce 31%\n",
      "2022-02-20 15:40:21,836 INFO mapreduce.Job:  map 100% reduce 32%\n",
      "2022-02-20 15:40:22,839 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2022-02-20 15:40:27,887 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "2022-02-20 15:40:28,904 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "2022-02-20 15:40:29,913 INFO mapreduce.Job:  map 100% reduce 36%\n",
      "2022-02-20 15:40:33,957 INFO mapreduce.Job:  map 100% reduce 37%\n",
      "2022-02-20 15:40:34,973 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "2022-02-20 15:40:37,989 INFO mapreduce.Job:  map 100% reduce 39%\n",
      "2022-02-20 15:40:40,100 INFO mapreduce.Job:  map 100% reduce 40%\n",
      "2022-02-20 15:40:42,109 INFO mapreduce.Job:  map 100% reduce 41%\n",
      "2022-02-20 15:40:46,136 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2022-02-20 15:40:51,150 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "2022-02-20 15:40:52,158 INFO mapreduce.Job:  map 100% reduce 44%\n",
      "2022-02-20 15:40:53,162 INFO mapreduce.Job:  map 100% reduce 46%\n",
      "2022-02-20 15:40:57,174 INFO mapreduce.Job:  map 100% reduce 47%\n",
      "2022-02-20 15:40:59,181 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "2022-02-20 15:41:03,191 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2022-02-20 15:41:05,197 INFO mapreduce.Job:  map 100% reduce 51%\n",
      "2022-02-20 15:41:06,201 INFO mapreduce.Job:  map 100% reduce 52%\n",
      "2022-02-20 15:41:10,212 INFO mapreduce.Job:  map 100% reduce 53%\n",
      "2022-02-20 15:41:11,216 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "2022-02-20 15:41:15,239 INFO mapreduce.Job:  map 100% reduce 55%\n",
      "2022-02-20 15:41:17,244 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "2022-02-20 15:41:22,258 INFO mapreduce.Job:  map 100% reduce 64%\n",
      "2022-02-20 15:41:23,261 INFO mapreduce.Job:  map 100% reduce 65%\n",
      "2022-02-20 15:41:28,338 INFO mapreduce.Job:  map 100% reduce 66%\n",
      "2022-02-20 15:41:29,351 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2022-02-20 15:41:31,385 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "2022-02-20 15:41:32,397 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2022-02-20 15:41:34,406 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "2022-02-20 15:41:35,411 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "2022-02-20 15:41:38,431 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "2022-02-20 15:41:40,442 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "2022-02-20 15:41:41,459 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2022-02-20 15:41:44,530 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "2022-02-20 15:41:46,541 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "2022-02-20 15:41:47,544 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "2022-02-20 15:41:49,566 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "2022-02-20 15:41:52,582 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "2022-02-20 15:41:56,672 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "2022-02-20 15:41:58,677 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "2022-02-20 15:42:02,687 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "2022-02-20 15:42:04,692 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "2022-02-20 15:42:08,762 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "2022-02-20 15:42:10,768 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "2022-02-20 15:42:14,780 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "2022-02-20 15:42:19,793 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "2022-02-20 15:42:22,801 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "2022-02-20 15:42:25,810 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "2022-02-20 15:42:28,818 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "2022-02-20 15:42:31,825 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "2022-02-20 15:42:32,830 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "2022-02-20 15:42:37,843 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-02-20 15:42:38,849 INFO mapreduce.Job: Job job_1645357742880_0002 completed successfully\n",
      "2022-02-20 15:42:38,946 INFO mapreduce.Job: Counters: 57\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2292244196\n",
      "\t\tFILE: Number of bytes written=3360997467\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13187426144\n",
      "\t\tHDFS: Number of bytes written=42110813\n",
      "\t\tHDFS: Number of read operations=200\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=30\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=50\n",
      "\t\tLaunched reduce tasks=11\n",
      "\t\tData-local map tasks=47\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18463743\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9860202\n",
      "\t\tTotal time spent by all map tasks (ms)=6154581\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3286734\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6154581\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3286734\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18906872832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10096846848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=286821375\n",
      "\t\tMap output records=286821375\n",
      "\t\tMap output bytes=5949896406\n",
      "\t\tMap output materialized bytes=1101724816\n",
      "\t\tInput split bytes=7250\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3346720\n",
      "\t\tReduce shuffle bytes=1101724816\n",
      "\t\tReduce input records=286821375\n",
      "\t\tReduce output records=3346720\n",
      "\t\tSpilled Records=859862054\n",
      "\t\tShuffled Maps =500\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=500\n",
      "\t\tGC time elapsed (ms)=18025\n",
      "\t\tCPU time spent (ms)=7096090\n",
      "\t\tPhysical memory (bytes) snapshot=34047418368\n",
      "\t\tVirtual memory (bytes) snapshot=260441538560\n",
      "\t\tTotal committed heap usage (bytes)=39124992000\n",
      "\t\tPeak Map Physical memory (bytes)=510541824\n",
      "\t\tPeak Map Virtual memory (bytes)=4383035392\n",
      "\t\tPeak Reduce Physical memory (bytes)=1401036800\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4369170432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13187418894\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42110813\n",
      "2022-02-20 15:42:38,947 INFO streaming.StreamJob: Output directory: /user/visits/result/\n",
      "CPU times: user 35.2 s, sys: 8.23 s, total: 43.4 s\n",
      "Wall time: 32min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/visits/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"longest-session\" \\\n",
    "-D mapreduce.job.reduces=10 \\\n",
    "-files ~/mhw2/longest-session.py \\\n",
    "-mapper \"python3 longest-session.py map\" \\\n",
    "-reducer \"python3 longest-session.py reduce\" \\\n",
    "-input /user/visits/data/ \\\n",
    "-output /user/visits/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "import sys\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        values = value.strip()\n",
    "        value, _ = value.split('.')\n",
    "        print(\"{}+{}\\t\".format(key, value))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        user, duration = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(user, duration))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/visits/top10\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7945251626992603150.jar tmpDir=null\n",
      "2022-02-20 15:51:18,581 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:8032\n",
      "2022-02-20 15:51:18,799 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:10200\n",
      "2022-02-20 15:51:18,837 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:8032\n",
      "2022-02-20 15:51:18,838 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net/10.128.0.31:10200\n",
      "2022-02-20 15:51:19,061 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1645357742880_0006\n",
      "2022-02-20 15:51:19,383 INFO mapred.FileInputFormat: Total input files to process : 10\n",
      "2022-02-20 15:51:20,276 INFO mapreduce.JobSubmitter: number of splits:20\n",
      "2022-02-20 15:51:20,861 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1645357742880_0006\n",
      "2022-02-20 15:51:20,863 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-02-20 15:51:21,083 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-02-20 15:51:21,084 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-02-20 15:51:21,177 INFO impl.YarnClientImpl: Submitted application application_1645357742880_0006\n",
      "2022-02-20 15:51:21,225 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-5ohwrulcy2ee6jxr.mdb.yandexcloud.net:8088/proxy/application_1645357742880_0006/\n",
      "2022-02-20 15:51:21,227 INFO mapreduce.Job: Running job: job_1645357742880_0006\n",
      "2022-02-20 15:51:28,335 INFO mapreduce.Job: Job job_1645357742880_0006 running in uber mode : false\n",
      "2022-02-20 15:51:28,336 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-02-20 15:51:37,520 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2022-02-20 15:51:46,659 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "2022-02-20 15:51:54,710 INFO mapreduce.Job:  map 65% reduce 15%\n",
      "2022-02-20 15:52:00,752 INFO mapreduce.Job:  map 65% reduce 22%\n",
      "2022-02-20 15:52:02,764 INFO mapreduce.Job:  map 80% reduce 22%\n",
      "2022-02-20 15:52:03,771 INFO mapreduce.Job:  map 85% reduce 22%\n",
      "2022-02-20 15:52:06,789 INFO mapreduce.Job:  map 85% reduce 28%\n",
      "2022-02-20 15:52:08,803 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "2022-02-20 15:52:12,830 INFO mapreduce.Job:  map 100% reduce 48%\n",
      "2022-02-20 15:52:16,925 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-02-20 15:52:17,941 INFO mapreduce.Job: Job job_1645357742880_0006 completed successfully\n",
      "2022-02-20 15:52:18,052 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10776661\n",
      "\t\tFILE: Number of bytes written=28473884\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=43340473\n",
      "\t\tHDFS: Number of bytes written=128\n",
      "\t\tHDFS: Number of read operations=65\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=20\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=19\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=415062\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=111111\n",
      "\t\tTotal time spent by all map tasks (ms)=138354\n",
      "\t\tTotal time spent by all reduce tasks (ms)=37037\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=138354\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=37037\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=425023488\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=113777664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3346720\n",
      "\t\tMap output records=3346720\n",
      "\t\tMap output bytes=38764093\n",
      "\t\tMap output materialized bytes=12619363\n",
      "\t\tInput split bytes=2820\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3346720\n",
      "\t\tReduce shuffle bytes=12619363\n",
      "\t\tReduce input records=3346720\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=6693440\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=4194\n",
      "\t\tCPU time spent (ms)=68930\n",
      "\t\tPhysical memory (bytes) snapshot=9960202240\n",
      "\t\tVirtual memory (bytes) snapshot=91101835264\n",
      "\t\tTotal committed heap usage (bytes)=9878634496\n",
      "\t\tPeak Map Physical memory (bytes)=501424128\n",
      "\t\tPeak Map Virtual memory (bytes)=4341022720\n",
      "\t\tPeak Reduce Physical memory (bytes)=731975680\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4333961216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=43337653\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=128\n",
      "2022-02-20 15:52:18,053 INFO streaming.StreamJob: Output directory: /user/visits/top10/\n",
      "CPU times: user 1.16 s, sys: 297 ms, total: 1.46 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/visits/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/visits/result/ \\\n",
    "-output /user/visits/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2022-02-20 15:52 /user/visits/top10/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        128 2022-02-20 15:52 /user/visits/top10/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/visits/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:19:42,838 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 17:19:42,929 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 17:19:42,929 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 17:19:46,548 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 17:19:46,548 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 17:19:46,548 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/visits/top10/part-* s3a://bucket4lsml/mhw2_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3492618\t5015\r\n",
      "1305201\t4521\r\n",
      "1978363\t4429\r\n",
      "1813207\t3973\r\n",
      "3719053\t3722\r\n",
      "164411\t3302\r\n",
      "576329\t3302\r\n",
      "2574666\t3301\r\n",
      "1649546\t3218\r\n",
      "3320567\t3156\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/visits/top10/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. [+5 баллов]** В этой секции будет работать с большим количеством таблиц, которые есть в датасете. Подробное описание данных в этих таблицах и их взаимосвязей есть на странице Kaggle - https://www.kaggle.com/c/avito-context-ad-clicks/data\n",
    "\n",
    "Схема данных следующая \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/4438/media/DB_schema.png\">\n",
    "\n",
    "Для данных нужно подсчитать некоторые статистики.\n",
    "\n",
    "\n",
    "**Важно!** Результаты каждого из 5 пунктов сохраните в виде файла в облачное хранилище. Ссылки на все 5 файлов должны быть указаны в работе. \n",
    "\n",
    "Итого, в ноутбуке должны присутствовать \n",
    "\n",
    "1) Ячейки с кодом на Spark\n",
    "\n",
    "2) Ссылки на все файлы в облаке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Подгрузим все файлы:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"hw2\")\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo -u hdfs hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/avito\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/avito\n",
    "! hdfs dfs -mkdir -p /user/avito\n",
    "\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/SearchInfo.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/testSearchStream.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/trainSearchStream.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/AdsInfo.tsv /user/avito/\n",
    "! hdfs dfs -put /home/ubuntu/avito-dataset/VisitsStream.tsv /user/avito/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/avito/parquet': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/avito/parquet\n",
    "! hdfs dfs -mkdir -p /user/avito/parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info_df = se.read.option(\"mode\",\"DROPMALFORMED\") \\\n",
    "                .option('sep', \"\\t\") \\\n",
    "                .csv(\"/user/avito/SearchInfo.tsv\", header=True, inferSchema=True)\n",
    "search_info_df.write.parquet(\"/user/avito/parquet/SearchInfo.parquet\")\n",
    "\n",
    "stream_train_df = se.read.option(\"mode\",\"DROPMALFORMED\") \\\n",
    "                .option('sep', \"\\t\") \\\n",
    "                .csv(\"/user/avito/trainSearchStream.tsv\", header=True, inferSchema=True)\n",
    "stream_train_df.write.parquet(\"/user/avito/parquet/trainSearchStream.parquet\")\n",
    "\n",
    "\n",
    "ads_info_df = se.read.option(\"mode\",\"DROPMALFORMED\") \\\n",
    "                .option('sep', \"\\t\") \\\n",
    "                .csv(\"/user/avito/AdsInfo.tsv\", header=True, inferSchema=True)\n",
    "\n",
    "ads_info_df.write.parquet(\"/user/avito/parquet/AdsInfo.parquet\")\n",
    "\n",
    "visits_df = se.read.option(\"mode\",\"DROPMALFORMED\") \\\n",
    "                .option('sep', \"\\t\") \\\n",
    "                .csv(\"/user/avito/VisitsStream.tsv\", header=True, inferSchema=True)\n",
    "\n",
    "visits_df.write.parquet(\"/user/avito/parquet/VisitsStream.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info = se.read.parquet(\"/user/avito/parquet/SearchInfo.parquet\")\n",
    "search_info.registerTempTable('search_info')\n",
    "\n",
    "stream_df = se.read.parquet(\"/user/avito/parquet/trainSearchStream.parquet\")\n",
    "stream_df.registerTempTable('stream_train')\n",
    "\n",
    "ads_df = se.read.parquet(\"/user/avito/parquet/AdsInfo.parquet\")\n",
    "ads_df.registerTempTable('ads')\n",
    "\n",
    "visits_df = se.read.parquet(\"/user/avito/parquet/VisitsStream.parquet\")\n",
    "visits_df.registerTempTable('visits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. [1 балл]** Найти топ 10 самых популярных фильтров. Фильтры кодируются ключами в словаре SearchParams (именно ключи (числа), а не значения). Задача внезапно творческая и будут приниматься любые разумные подходы (aka костыли), которые решат задачу. Удачи :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_not_null = se.sql(\"\"\"\n",
    "    SELECT\n",
    "        SearchParams\n",
    "    FROM search_info\n",
    "    WHERE SearchParams IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        SearchParams|\n",
      "+--------------------+\n",
      "|{83:'Обувь', 175:...|\n",
      "|  {175:'Аксессуары'}|\n",
      "|      {156:'Горные'}|\n",
      "|{45:'Кровати, див...|\n",
      "|{45:'Кровати, див...|\n",
      "|{5:'Шины, диски и...|\n",
      "|{110:'Верхняя оде...|\n",
      "|{45:'Подставки и ...|\n",
      "|{127:'Детские кол...|\n",
      "|{45:'Кухонные гар...|\n",
      "|{178:'Для мальчик...|\n",
      "|{127:'Детская меб...|\n",
      "|{124:'32', 110:'О...|\n",
      "|   {223:'Объективы'}|\n",
      "|{83:'Обувь', 175:...|\n",
      "|{486:'Микроволнов...|\n",
      "|{44:'Сантехника и...|\n",
      "|  {44:'Инструменты'}|\n",
      "|{83:'Верхняя одеж...|\n",
      "|{487:'Швейные маш...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params_not_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filters(text):\n",
    "    text = text.SearchParams[1:-1]\n",
    "    pattern = re.compile(r\"[\\d]+:\\'(.*?)\\'\")\n",
    "    result = []\n",
    "    \n",
    "    for match in pattern.finditer(text.lower()):\n",
    "        key_val = match.group(0)\n",
    "        result.append(key_val)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_splitted = params_not_null.rdd.flatMap(extract_filters).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"83:'обувь'\",\n",
       " \"175:'женская одежда'\",\n",
       " \"88:'38'\",\n",
       " \"175:'аксессуары'\",\n",
       " \"156:'горные'\"]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_splitted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/avito/results/first': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/avito/results/first || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    params_splitted\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] < 10)\n",
    "    .map(lambda x: x[0][1].split(':')[1])\n",
    "    .saveAsTextFile('/user/avito/results/first')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 20:01:04,761 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 20:01:04,851 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 20:01:04,852 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 20:01:08,301 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 20:01:08,302 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 20:01:08,302 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/avito/results/first/part-00000 s3a://bucket4lsml/mhw2_2_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'женская одежда'\r\n",
      "'шины, диски и колёса'\r\n",
      "'для девочек'\r\n",
      "'обувь'\r\n",
      "'для мальчиков'\r\n",
      "'детские коляски'\r\n",
      "'запчасти'\r\n",
      "'кровати, диваны и кресла'\r\n",
      "'мужская одежда'\r\n",
      "'для автомобилей'\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/avito/results/first/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. [1 балл]** Найдите топ 10 самых встречаемых слов в запросах, в которых пользователь кликнул по рекламе. Слова должны быть приведены к нижнеу регистру. \n",
    "\n",
    "Для примера\n",
    "\n",
    "```bash\n",
    "\"Купить стол\" -> Кликнул\n",
    "\"Ноутбук\" -> Кликнул\n",
    "\"Купить машину\" -> Кликнул\n",
    "\"Красивый стол\" -> Не кликнул\n",
    "\"Большой стол\" -> Кликнул\n",
    "\"Купить маску\" -> Кликнул\n",
    "```\n",
    "\n",
    "Топ слов (с указанием того, сколько оно встретилось)\n",
    "\n",
    "```bash\n",
    "купить - 3\n",
    "стол - 2\n",
    "ноутбук - 1\n",
    "большой - 1\n",
    "маску - 1\n",
    "машину - 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_clicked = se.sql(\"\"\"\n",
    "    SELECT \n",
    "        r.SearchQuery\n",
    "    FROM\n",
    "        stream_train l\n",
    "        JOIN search_info r ON l.SearchID = r.SearchID\n",
    "    WHERE\n",
    "        l.IsClick = 1 AND r.SearchQuery IS NOT NULL\n",
    "\"\"\").registerTempTable('clicked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         SearchQuery|\n",
      "+--------------------+\n",
      "|             монопод|\n",
      "|сварочный аппарат бу|\n",
      "|     iphone 5s новый|\n",
      "|            ваз 2110|\n",
      "|               туфли|\n",
      "|             счетчик|\n",
      "|  микроволновая печь|\n",
      "|            мокасины|\n",
      "|  продажа велосипеда|\n",
      "|бампер ниссан при...|\n",
      "|           huawei m1|\n",
      "|         мультиварка|\n",
      "|             кровать|\n",
      "|             коляска|\n",
      "|    видеорегистратор|\n",
      "|    видеорегистратор|\n",
      "|                шкаф|\n",
      "|toyota land cruis...|\n",
      "|        babyliss pro|\n",
      "|     тумба для обуви|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# queries_clicked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_из интереса в этой задаче попробовала двумя способами_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "import string\n",
    "\n",
    "def wordify(text):\n",
    "    pattern = re.compile(r\"[\\w]+\")\n",
    "    result = []\n",
    "\n",
    "    for match in pattern.finditer(text.lower()):\n",
    "        word = match.group(0)\n",
    "        result.append(word)\n",
    "    return '\\t'.join(result)\n",
    "\n",
    "f_wordify = se.udf.register(\"wordify\", wordify, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      word|frequency|\n",
      "+----------+---------+\n",
      "| велосипед|    11417|\n",
      "|        бу|    10418|\n",
      "|    iphone|     6891|\n",
      "|       для|     5508|\n",
      "|     диван|     5254|\n",
      "|        на|     4590|\n",
      "|    платье|     4021|\n",
      "|велосипеды|     3443|\n",
      "|   коляска|     3421|\n",
      "|   самокат|     3208|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_words_clicked = se.sql(\"\"\"\n",
    "    SELECT\n",
    "        word,\n",
    "        COUNT(word) as frequency\n",
    "    FROM (\n",
    "        SELECT\n",
    "            explode(split(wordify(SearchQuery), '\\t')) as word\n",
    "        FROM\n",
    "            clicked\n",
    "    )\n",
    "    GROUP BY word\n",
    "    ORDER BY frequency DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "top_words_clicked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    pattern = re.compile(r\"[\\w]+\")\n",
    "    result = []\n",
    "\n",
    "    for match in pattern.finditer(text.lower()):\n",
    "        word = match.group(0)\n",
    "        result.append(word)\n",
    "    return result\n",
    "\n",
    "words = queries_clicked.rdd.flatMap(lambda x: extract_words(x['SearchQuery'])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['монопод', 'сварочный', 'аппарат', 'бу', 'iphone']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/avito/results/second\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/avito/results/second || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    words\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] < 10)\n",
    "    .map(lambda x: f'{x[0][1]} - {x[0][0]}')\n",
    "    .saveAsTextFile('/user/avito/results/second')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 20:02:26,331 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 20:02:26,425 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 20:02:26,425 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 20:02:29,835 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 20:02:29,835 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 20:02:29,835 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/avito/results/second/part-00000 s3a://bucket4lsml/mhw2_2_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_2_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "велосипед - 11417\r\n",
      "бу - 10418\r\n",
      "iphone - 6891\r\n",
      "для - 5508\r\n",
      "диван - 5254\r\n",
      "на - 4590\r\n",
      "платье - 4021\r\n",
      "велосипеды - 3443\r\n",
      "коляска - 3421\r\n",
      "самокат - 3208\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/avito/results/second/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. [1 балл]** Для каждого слова из заголовка объявления подсчитать его среднюю стоимость. Средняя стоимость слова - это среднее стоимости всех объявлений, где оно встретилось. Так например если слово появилось в заголовке рекламы с ценой A и в заголовке рекламы с ценой B, то его средняя стоимость - (A+B)/2 . Слова должны быть приведены к нижнему регистру.\n",
    "\n",
    "Учитывать необходимо только записи, где есть указаная стоимость и заголовок.\n",
    "\n",
    "В качестве ответа - топ 10 самых дорогих слов с указанием их средней стоимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Воспользуемся udf-м из предыдущей задачи:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = se.sql(\"\"\"\n",
    "    WITH titles_by_words AS (\n",
    "        SELECT\n",
    "            explode(split(wordify(Title), '\\t')) as word,\n",
    "            Price,\n",
    "            AdID\n",
    "        FROM\n",
    "            ads\n",
    "        WHERE\n",
    "            Title IS NOT NULL AND Price IS NOT NULL\n",
    "    )\n",
    "    \n",
    "    SELECT\n",
    "        word,\n",
    "        mean(Price) as mean_price\n",
    "    FROM titles_by_words\n",
    "    WHERE word IS NOT NULL\n",
    "    GROUP BY\n",
    "        word, AdID\n",
    "    ORDER BY mean_price DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.registerTempTable('titles_with_clicks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mean_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>куплю</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аптеку</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>промназначения</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>с</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>маникюра</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>сниму</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>производственное</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>м²</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>трон</td>\n",
       "      <td>1.000000e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word    mean_price\n",
       "0             куплю  1.000000e+12\n",
       "1            аптеку  1.000000e+12\n",
       "2    промназначения  1.000000e+12\n",
       "3                 5  1.000000e+12\n",
       "4                 с  1.000000e+12\n",
       "5          маникюра  1.000000e+12\n",
       "6             сниму  1.000000e+12\n",
       "7  производственное  1.000000e+12\n",
       "8                м²  1.000000e+12\n",
       "9              трон  1.000000e+12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \"\"\"\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \"\"\"\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# title_words = se.sql(\"\"\"\n",
    "#     SELECT\n",
    "#         explode(split(wordify(Title), '\\t')) as word,\n",
    "#         Price,\n",
    "#         AdID\n",
    "#     FROM\n",
    "#         ads\n",
    "#     WHERE\n",
    "#         Title IS NOT NULL AND Price IS NOT NULL\n",
    "# \"\"\").rdd.cache()\n",
    "\n",
    "# %%time\n",
    "# (\n",
    "#     title_words\n",
    "#     .map(lambda x: (x[0], (x[1], 1)))\n",
    "#     .reduceByKey(lambda a, b: a + b)\n",
    "#     .map(lambda x: (x[1][0] / x[1][1], x[0]))\n",
    "#     .sortByKey(ascending=False)\n",
    "#     .map(lambda x: f'{x[1]} - {x[0]}')\n",
    "#     .saveAsTextFile('/user/avito/results/third_2.txt') \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_rdd = titles.rdd.map(lambda x: '{}-{}'.format(x[0], x[1])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1960-999999999999.0',\n",
       " 'помещение-999999999999.0',\n",
       " 'м²-999999999999.0',\n",
       " '5-999999999999.0',\n",
       " 'помещение-999999999999.0',\n",
       " 'м²-999999999999.0',\n",
       " 'складское-999999999999.0',\n",
       " 'квартиру-999999999999.0',\n",
       " 'на-999999999999.0',\n",
       " 'участок-999999999999.0']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_rdd.saveAsTextFile(\"/user/avito/results/third.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:57:32,152 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 17:57:32,249 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 17:57:32,249 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 17:57:36,158 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 17:57:36,159 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 17:57:36,159 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/avito/results/third.txt/part-00000 s3a://bucket4lsml/mhw2_2_3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_2_3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960-999999999999.0\r\n",
      "помещение-999999999999.0\r\n",
      "м²-999999999999.0\r\n",
      "5-999999999999.0\r\n",
      "помещение-999999999999.0\r\n",
      "м²-999999999999.0\r\n",
      "складское-999999999999.0\r\n",
      "квартиру-999999999999.0\r\n",
      "на-999999999999.0\r\n",
      "участок-999999999999.0\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/avito/results/third.txt/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отфильтровывать стоп-слова?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 [1 балл]** Найдите всех пользователей, которые заходили каждый день на протяжении всего времени измерений. В качестве ответа запишите одно число - количество этих пользвателей.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Уберем время из даты и оставим только дни: YYYY-MM-DD_\n",
    "\n",
    "_И уберем дупликаты, чтобы в дальнейшем считать уникальные значения_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_day(text):\n",
    "    return text.split(' ')[0]\n",
    "\n",
    "daytify = se.udf.register(\"daytify\", take_day, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_days = se.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        UserID,\n",
    "        daytify(ViewDate) as Day\n",
    "    FROM visits\n",
    "\"\"\")\n",
    "visits_days.registerTempTable(\"days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_rdd = visits_days.rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Найдем количество уникальных дней_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_days_num = visits_days.select('Day').distinct().rdd.map(lambda x: x.Day).count()\n",
    "unique_days_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Теперь сгруппируем по юзерам, посчитаем число дней и сравним с unique_days_num_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.5 ms, sys: 35.1 ms, total: 79.7 ms\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "active_users_list = (\n",
    "    visits_rdd\n",
    "    .map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] == unique_days_num)\n",
    "    .map(lambda x: x[0])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_users = se.sql(\"\"\"\n",
    "#     SELECT\n",
    "#         UserID\n",
    "#     FROM (\n",
    "#         SELECT\n",
    "#             UserID,\n",
    "#             count(Day) as num_days\n",
    "#         FROM days\n",
    "#         GROUP BY\n",
    "#             UserID\n",
    "#     )\n",
    "#     WHERE num_days = {}\n",
    "# \"\"\".format(unique_days_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "res24 = sc.parallelize([len(active_users_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res24.saveAsTextFile(\"/user/avito/results/fourth.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 18:26:42,730 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 18:26:42,832 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 18:26:42,832 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 18:26:46,429 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 18:26:46,429 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 18:26:46,430 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/avito/results/fourth.txt/part-00003 s3a://bucket4lsml/mhw2_2_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_2_4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/avito/results/fourth.txt/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 [1 балл]** Для каждого дня найдите количество уникальных пользователей, которые заходили на сайт в этот день. Выкиньте из рассмотрения всех пользователей, которые вы нашли в пункте 4 (то есть тех, которые заходили каждый день какого-либо месяца). \n",
    "\n",
    "В ответе укажите пары день-число уникальных пользователей в порядке убывания количества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_users_set = set(active_users_list)\n",
    "\n",
    "def remove_active(user_id):\n",
    "    return user_id not in active_users_set\n",
    "\n",
    "nonactive = se.udf.register(\"nonactive\", remove_active, \"boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct as countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Воспользуемся табличкой из предыдущего пункта и уберем слишком активных юзеров_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_users = visits_days.select('Day', 'UserID').filter(nonactive(F.col('UserID'))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|       Day| UserID|\n",
      "+----------+-------+\n",
      "|2015-05-19|3339459|\n",
      "|2015-05-19|3472056|\n",
      "|2015-05-19|3498500|\n",
      "|2015-05-19|3581440|\n",
      "|2015-05-19|3756375|\n",
      "|2015-05-19|3891751|\n",
      "|2015-05-19|3971739|\n",
      "|2015-05-19|4006974|\n",
      "|2015-05-19|4102599|\n",
      "|2015-05-19|4106979|\n",
      "|2015-05-19|4203357|\n",
      "|2015-05-19|4271481|\n",
      "|2015-05-19|  29832|\n",
      "|2015-05-19| 106382|\n",
      "|2015-05-19| 232031|\n",
      "|2015-05-19| 263551|\n",
      "|2015-05-19| 296159|\n",
      "|2015-05-19| 323952|\n",
      "|2015-05-19| 384101|\n",
      "|2015-05-19| 399210|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basic_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 53.4 ms, total: 159 ms\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collect_users_sql = (\n",
    "    basic_users.rdd\n",
    "    .map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .map(lambda x: f'{x[1]} - {x[0]}')\n",
    "    .saveAsTextFile('/user/avito/results/fifth_2.txt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_by_day = basic_users.select('Day', 'UserID').groupBy('Day').agg(countDistinct(basic_users.UserID)).cache()\n",
    "# count_users = users_by_day.sort(\"count(UserID)\", ascending=False).cache()\n",
    "# count_users.toPandas()\n",
    "# count_users.rdd.saveAsTextFile(\"/user/avito/results/fifth.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/avito/results/fifth_2.txt/* > tmp.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-21 20:07:44,027 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-02-21 20:07:44,113 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-02-21 20:07:44,113 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-02-21 20:07:46,834 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-02-21 20:07:46,834 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-02-21 20:07:46,835 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -put tmp.txt s3a://bucket4lsml/mhw2_2_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "https://storage.yandexcloud.net/bucket4lsml/mhw2_2_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-12 - 973248\n",
      "2015-05-13 - 847446\n",
      "2015-05-14 - 745904\n",
      "2015-05-06 - 733027\n",
      "2015-05-05 - 731624\n",
      "2015-05-07 - 729708\n",
      "2015-05-11 - 706240\n",
      "2015-04-27 - 676433\n",
      "2015-05-08 - 664982\n",
      "2015-04-28 - 655791\n",
      "2015-04-29 - 641148\n",
      "2015-05-15 - 633132\n",
      "2015-05-04 - 607205\n",
      "2015-04-30 - 596224\n",
      "2015-05-10 - 562151\n",
      "2015-04-26 - 540051\n",
      "2015-05-03 - 511687\n",
      "2015-04-25 - 498069\n",
      "2015-05-16 - 493577\n",
      "2015-05-18 - 488250\n",
      "2015-05-17 - 480968\n",
      "2015-05-02 - 468010\n",
      "2015-05-01 - 452800\n",
      "2015-05-09 - 450135\n",
      "2015-05-19 - 367777\n",
      "2015-05-20 - 184972\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/avito/results/fifth_2.txt/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно!** Следите за объемом потребляемой памяти! За решения, которые работают не оптимально по памяти, можно терять баллы. Понятное дело, что на текущих объемах скорее всего сработает примерно любое решение, но это не повод плохо писать алгоритм.\n",
    "\n",
    "Если в вашем алгоритме есть спорный момент в отношении использования памяти, но вы сделали это намерено - напишите явно в комментарии, что это осознаное решение, которое вы приняли по такой-то причине. Например вы могли запустить отдельную MR\\Spark задачу, которая бы показала, что во всем датасете определенного типа данных не более чем `M`, а значит мы не упремся в ограничения по памяти и вполне уместно использовать для его обработки именно такой подход.\n",
    "\n",
    "Если результат работы вашего алгоритма \"размазался\" по нескольким партам, то нужно дополнительно склеить их в один файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
